{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b52a3a40",
   "metadata": {},
   "source": [
    "# **Generate T2_PD H5 files of IXI dataset:**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef419a9",
   "metadata": {},
   "source": [
    "## **Import libraries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec2faa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch-summary\n",
    "# pip install tqdm\n",
    "# !pip install monai\n",
    "# !pip install h5py\n",
    "# !pip install pandas\n",
    "# !pip install scikit-image\n",
    "# !pip install voila"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e2c8553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ants\n",
    "import SimpleITK as sitk\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sys.path.insert(0,'./basics/')\n",
    "import basics.slice_view as slice_view\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# torchsummary and torchvision\n",
    "from torchsummary import summary\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# matplotlib stuff\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "\n",
    "# numpy and pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Common python packages\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import pathlib\n",
    "import random\n",
    "\n",
    "import h5py\n",
    "# from torch.utils.data import Dataset\n",
    "import torch\n",
    "from skimage import feature\n",
    "# from utils import npComplexToTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7778ec3a",
   "metadata": {},
   "source": [
    "## **Visualizing:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fae9306a",
   "metadata": {},
   "outputs": [],
   "source": [
    "link_IXI_t2='/media/Data/MRI/datasets/dataset_creation/IXI/niifiles/t2_nii/'\n",
    "link_IXI_pd='/media/Data/MRI/datasets/dataset_creation/IXI/niifiles/pd_nii/'\n",
    "link_IXI_t1='/media/Data/MRI/datasets/dataset_creation/IXI/niifiles/t1_nii/'\n",
    "out_link='/media/Data/MRI/datasets/translation_imputation/ixi_t2_pd/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "191bdc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(link):\n",
    "    img=sitk.ReadImage(link)\n",
    "    img2=sitk.GetArrayFromImage(img)\n",
    "    slv1 = slice_view.slicer(img2)\n",
    "    slv1.slicer_view()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "849a1cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "990a775fa21241c194d6dcd291c4a36b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='slice_view', options=('x', 'y', 'z'), value='x'), IntSlider(value=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(link_IXI_pd+'IXI016-Guys-0697-PD.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a9fba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "img=sitk.ReadImage(link_IXI_t2+'IXI016-Guys-0697-T2.nii.gz')\n",
    "img2=sitk.GetArrayFromImage(img)\n",
    "print(img2.shape)\n",
    "type(img2)\n",
    "plt.imshow(img2[64,:,:],'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fa6834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69a9af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = sitk.ReadImage(link_IXI_t1+'IXI002-Guys-0828-T1.nii.gz')\n",
    "img2= sitk.GetArrayFromImage(img)\n",
    "print(img2.shape)\n",
    "type(img2)\n",
    "plt.imshow(img2[90,:,:],'gray')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfebc5d0",
   "metadata": {},
   "source": [
    "## **Data_loader:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1977c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from monai.data import ITKReader,NibabelReader\n",
    "from monai.transforms import LoadImage, LoadImaged\n",
    "from monai.transforms import (\n",
    "    Orientationd, AddChanneld, Compose, ToTensord, Spacingd,Resized,ScaleIntensityD,ResizeWithPadOrCropd\n",
    "    # ScaleIntensityD, ScaleIntensityRangeD, AdjustContrastD, RandAffined, ToNumpyd,RepeatChannelD\n",
    ")\n",
    "from monai.data import Dataset\n",
    "import h5py\n",
    "\n",
    "class H5CachedDataset(Dataset):\n",
    "    def __init__(self,niilist,masklist1,\n",
    "                 transforms_offline, transforms_online=ToTensord(('T2',)),\n",
    "                 nslices_per_image = 130 ,\n",
    "                 start_slice = 0,\n",
    "                 end_slice = 129,                \n",
    "                 h5cachedir=None):\n",
    "        #### nslices_per_image ---> total slice in the volume\n",
    "        #### h5cachedir ---> directory to save one .h5 files for each volume & it would act like a cache directory\n",
    "        #### if h5cachedir does not exist create one \n",
    "#         print(transforms_online)\n",
    "\n",
    "        if h5cachedir is not None:\n",
    "            if not os.path.exists(h5cachedir):\n",
    "                os.mkdir(h5cachedir)\n",
    "            self.cachedir = h5cachedir\n",
    "        #### datalist ---> a list [{'image': volume_1_path},......,{'image': volume_n_path}]\n",
    "        #### masklist ---> a list [{'mask': mask_1_path},......,{'image': mask_n_path}]\n",
    "#         self.id=id_\n",
    "        self.datalist = niilist\n",
    "        self.masklist1 = masklist1\n",
    "        \n",
    "        self.xfms = transforms_offline\n",
    "        self.xfms2 = transforms_online\n",
    "        #### 3d image loader from monai\n",
    "        self.loader = LoadImage()\n",
    "        self.loader.register(NibabelReader())  \n",
    "        #### start_slice & end_slice---> slices to be truncated in each volume vol[:,:,start_slice:-end_slice]\n",
    "        self.start_slice = start_slice\n",
    "        self.end_slice = end_slice\n",
    "        #### nslices ---> nslices_per_image - end_slice i.e. slice value after end truncation\n",
    "        #            ---> nslices is kept flexible so that index is obtained by adding front truncation value &\n",
    "        #            ---> total length of the loder is caluclated considering subtracting front truncation value\n",
    "        self.nslices = nslices_per_image - self.end_slice\n",
    "        print(self.nslices) #13\n",
    "        \n",
    "    def __len__(self):\n",
    "        #### total number of slices in all the volumes\n",
    "        return len(self.datalist)*(self.nslices - self.start_slice)\n",
    "    \n",
    "    def clear_cache(self):\n",
    "        #### function to clear the directory storing h5 files (used for caching the h5 files)\n",
    "        for fn in os.listdir(self.cachedir):\n",
    "            os.remove(self.cachedir+'/'+fn)\n",
    "            \n",
    "    def __getitem__(self,index):\n",
    "        #### ditionary to store data slicewise\n",
    "        data = {}\n",
    "        #### index can take values from \n",
    "                # 0 to (total number of volumes * (len(datalist)*(nslices - start_slice)))  \n",
    "        #### filenum can take values from 0 to total number of volumes\n",
    "        #### slicenum can take values from 0 to (len(datalist)*(nslices - start_slice))\n",
    "        filenum = index // (self.nslices - self.start_slice)\n",
    "        slicenum = index % (self.nslices - self.start_slice)\n",
    "        slicenum += self.start_slice\n",
    "        \n",
    "        #### Extract the datafile location & mask file location based on filenum\n",
    "        datalist_filenum = self.datalist[filenum]\n",
    "        loc_data = datalist_filenum['T2']\n",
    "\n",
    "        masklist1_filenum = self.masklist1[filenum]\n",
    "        loc_mask1 = masklist1_filenum['PD']\n",
    "        print(filenum,slicenum)\n",
    "        \n",
    "        ##### if h5 exists for the current volume fill data dictionary with current slice number\n",
    "        if self.cachedir is not None:\n",
    "            h5name = self.cachedir+'/%d.h5' % filenum\n",
    "            ptname = self.cachedir+'/%d.pt' % filenum\n",
    "#             ptname = self.cachedir+'/%d.pt' % self.id\n",
    "\n",
    "            if os.path.exists(h5name):\n",
    "                with h5py.File(h5name,'r') as itm:\n",
    "                    for key in itm.keys():                       \n",
    "                        data[key]=torch.from_numpy(itm[key][:,:,:,slicenum])\n",
    "                data['image_meta_dict']={'affine':np.eye(3)} # FIXME: loses spacing info - can read from pt file\n",
    "\n",
    "        \n",
    "        ##### if data dictionary is empty\n",
    "        if len(data)==0:\n",
    "            #### Read image & mask data, meta data\n",
    "            imgdata, meta = self.loader(loc_data)\n",
    "            mask_data1, mask_meta1 = self.loader(loc_mask1)\n",
    "\n",
    "            #### store volume wise image & mask data,metadata in a dictionary \n",
    "            data_i = {'T2':imgdata,'PD':mask_data1, 'image_meta_dict':meta, 'label_meta_dict1':mask_meta1}\n",
    "            #### transform the data dictionary\n",
    "            data3d = self.xfms(data_i)\n",
    "            #### Create h5 file for the volume by chunking into the slice shape for data & mask \n",
    "            #### Create a .pt file for meta data\n",
    "            if self.cachedir is not None:\n",
    "                other = {}\n",
    "                with h5py.File(h5name,'w',libver='latest') as itm:\n",
    "                    for key in data3d:\n",
    "                        if key=='T2' or key=='PD':                             \n",
    "                            img_npy = data3d[key].numpy()\n",
    "                            shp = img_npy.shape\n",
    "                            chunk_size = list(shp[:-1])+[1]\n",
    "                            ds = itm.create_dataset(key,shp,chunks=tuple(chunk_size),dtype=img_npy.dtype)\n",
    "                            ds[:]=img_npy[:]\n",
    "                            \n",
    "                    else:\n",
    "                        other[key]=data3d[key]\n",
    "                torch.save(other,ptname)\n",
    "\n",
    "\n",
    "            #### fill the data dictionary\n",
    "            data = {\n",
    "                'T2':data3d['T2'][:,:,:,slicenum],\n",
    "                'PD':data3d['PD'][:,:,:,slicenum],\n",
    "                'image_meta_dict':{\n",
    "                    'affine':np.eye(3)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            \n",
    "#         print(self.__getitem__)\n",
    "        if len(data)>0:\n",
    "#             print(\"**\",data.keys())\n",
    "#             res = self.xfms2(data)\n",
    "            res = data\n",
    "            res['T2']=res['T2'].float()\n",
    "            res['PD']=res['PD'].float()\n",
    "            res['filenum'] = filenum\n",
    "            res['slicenum'] = slicenum\n",
    "            res['idx'] = index\n",
    "            return res\n",
    "\n",
    "        else:\n",
    "            # replace with random\n",
    "            return self.__getitem__(np.random.randint(len(self.datalist)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41592fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install nibabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c116a5eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53de12d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.data import ITKReader,NibabelReader\n",
    "from monai.transforms import LoadImage, LoadImaged\n",
    "from monai.transforms import (\n",
    "    Orientationd, AddChanneld, Compose, ToTensord, Spacingd,Resized,ScaleIntensityD,ResizeWithPadOrCropd\n",
    "    # ScaleIntensityD, ScaleIntensityRangeD, AdjustContrastD, RandAffined, ToNumpyd,RepeatChannelD\n",
    ")\n",
    "from monai.data import Dataset\n",
    "import h5py\n",
    "# root='/srv/project/APW/gayathri\n",
    "link_IXI_t2='/media/Data/MRI/datasets/dataset_creation/IXI/niifiles/t2_nii/'\n",
    "link_IXI_pd='/media/Data/MRI/datasets/dataset_creation/IXI/niifiles/pd_nii/'\n",
    "\n",
    "datadir=glob.glob(link_IXI_t2)[0] ###T2\n",
    "maskdir=glob.glob(link_IXI_pd)[0] ###DCE\n",
    "vol_file_ext = '.nii.gz'\n",
    "datalist = [{'T2':x} for x in glob.glob(datadir+'IXI**T2'+vol_file_ext) ]\n",
    "masklist1= [{'PD':x} for x in glob.glob(maskdir+'IXI**PD'+vol_file_ext) ]\n",
    "\n",
    "my_dict={}\n",
    "my_dict2={}\n",
    "for i in range(393):\n",
    "    d=datalist[i]['T2'].split('/')[-1].split('-T2')[:]\n",
    "    my_dict[d[0]] = '-T2'+d[1]\n",
    "    d2=masklist1[i]['PD'].split('/')[-1].split('-PD')[:]\n",
    "    my_dict2[d2[0]] = '-PD'+d2[1]\n",
    "# datalist = [{'T2':x} for x in datadir]l=[]\n",
    "l=[]\n",
    "k=[]\n",
    "for i in range(393):\n",
    "    l.append({'T2':link_IXI_t2+sorted(my_dict)[i]+my_dict[sorted(my_dict)[i]]})\n",
    "    k.append({'PD':link_IXI_pd+sorted(my_dict2)[i]+my_dict2[sorted(my_dict2)[i]]})\n",
    "\n",
    "transforms1 = Compose([AddChanneld(('T2','PD')),Orientationd(('T2','PD'),'RAS'),ToTensord(('T2','PD'))])\n",
    "\n",
    "### The loader is such that it would create h5 files if they are not created when the loader is called and executed\n",
    "h5cachedir_ = './IXI-dataset/T2_PD_H5' ###saving path                                                               \n",
    "h5cacheds = H5CachedDataset(l,k,transforms1,h5cachedir=h5cachedir_)\n",
    "\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "_loader = torch.utils.data.DataLoader(h5cacheds,batch_size = 1,\\\n",
    "                                           shuffle = False)# ,pin_memory = True)\n",
    "for i,batch in enumerate(_loader):\n",
    "    print(batch['T2'].shape)\n",
    "    print(batch['PD'].shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf417ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,batch in enumerate(_loader):\n",
    "#     print(batch['T2'].shape)\n",
    "#     print(batch['PD'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16373895",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dab9849",
   "metadata": {},
   "source": [
    "## **H5 Files:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec21a3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: <KeysViewHDF5 ['PD', 'T2']>\n",
      "T2\n",
      "(1, 256, 256, 130)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "filename = \"/home/gayathri/torchmri/Codes/IXI-dataset/T2_PD_H5/390.h5\"\n",
    "\n",
    "with h5py.File(filename, \"r\") as f:\n",
    "    # List all groups\n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "    a_group_key = list(f.keys())[1]\n",
    "    print(list(f.keys())[1])\n",
    "    print(f[a_group_key].shape)\n",
    "\n",
    "    # Get the data\n",
    "#     data = list(f[a_group_key])\n",
    "    data_T2 = np.asarray(f[a_group_key])\n",
    "\n",
    "    \n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c33283",
   "metadata": {},
   "source": [
    "## **Checking whether h5 files are correct:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c11fea07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(data_T2)\n",
    "data_T2.shape\n",
    "# d=squeeze(data_T2(1,:,:,:))\n",
    "d=data_T2.reshape(256, 256, 130)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a3f0e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f2f5090cfa243d389054a92513053f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='slice_view', options=('x', 'y', 'z'), value='x'), IntSlider(value=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdf1a5f80d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# img2=sitk.GetArrayFromImage(d)\n",
    "# slv1 = slice_view.slicer(sitk.GetArrayFromImage(sitk.ReadImage(data_T2))\n",
    "im1=d[:,:,100]\n",
    "i2=sitk.ReadImage(link_IXI_t2+'IXI652-Guys-1116-T2.nii.gz')\n",
    "im2=sitk.GetArrayFromImage(i2)\n",
    "imge2=im2[100,:,:]\n",
    "print(imge2.shape)\n",
    "slv1 = slice_view.slicer(np.transpose(d, (1, 0, 2)))\n",
    "slv1.slicer_view()\n",
    "\n",
    "plt.imshow(imge2-im1.transpose(),'gray')\n",
    "# bts=imge2-im1.transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a6107db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e54d0444eaf440b98fdc9c485566780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='slice_view', options=('x', 'y', 'z'), value='x'), IntSlider(value=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(link_IXI_t2+'IXI652-Guys-1116-T2.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e38539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k[390]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce72e5fa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d90500",
   "metadata": {},
   "source": [
    "# **MR Dataset- Flair:IR-->T1-->Flair:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ecd2502",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: <KeysViewHDF5 ['img_volus_4x', 'kspace_volus_4x', 'mask', 'volfs']>\n",
      "mask\n",
      "(240, 240, 48)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "# filename = \"/home/gayathri/torchmri/Codes/IXI-dataset/T2_PD_H5/390.h5\"\n",
    "filename = '/media/Data/MRI/datasets/mrbrain_t1/cartesian/train/acc_4x/4.h5'\n",
    "# filename = '/media/Data/MRI/datasets/mrbrain_ir/cartesian/train/acc_4x/4.h5'\n",
    "# filename = '/media/Data/MRI/datasets/mrbrain_flair/cartesian/train/acc_4x/4.h5'\n",
    "\n",
    "with h5py.File(filename, \"r\") as f:\n",
    "    # List all groups\n",
    "    print(\"Keys: %s\" % f.keys())\n",
    "#     a_group_key = list(f.keys())['img_volus_4x']\n",
    "    a=f['img_volus_4x']\n",
    "    b=f['kspace_volus_4x']\n",
    "    c=f['volfs']\n",
    "    print(list(f.keys())[2])\n",
    "    print(f[a_group_key].shape)\n",
    "\n",
    "    # Get the data\n",
    "#     data = list(f[a_group_key])\n",
    "    data_img = np.asarray(a)\n",
    "    data_k = np.asarray(b)\n",
    "    data_full = np.asarray(c)\n",
    "\n",
    "    \n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50f1f026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 240, 48)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27049874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # data_img.shape\n",
    "\n",
    "# import nibabel as nib\n",
    "\n",
    "# ni_img = nib.Nifti1Image(data_img, affine=np.eye(4))\n",
    "# nib.save(ni_img, \"volume_4ximage.nii\")\n",
    "# ni_img2 = nib.Nifti1Image(data_k, affine=np.eye(4))\n",
    "# nib.save(ni_img2, \"volume_kimage.nii\")\n",
    "# ni_img3 = nib.Nifti1Image(data_full, affine=np.eye(4))\n",
    "# nib.save(ni_img3, \"volume_fullimage.nii\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66bd74ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2701fc40a73416bbcc5b6bf6fbaa00e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='slice_view', options=('x', 'y', 'z'), value='x'), IntSlider(value=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function ipywidgets.widgets.interaction._InteractFactory.__call__.<locals>.<lambda>(*args, **kwargs)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slv1 = slice_view.slicer(data_t1)\n",
    "slv1.slicer_view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77dc4bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1abbaa8f7ed44e13b49d3709543d7d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='slice_view', options=('x', 'y', 'z'), value='x'), IntSlider(value=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function ipywidgets.widgets.interaction._InteractFactory.__call__.<locals>.<lambda>(*args, **kwargs)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_T1\n",
    "slv1 = slice_view.slicer(data_flr)\n",
    "slv1.slicer_view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9616986a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f92fe4012b4642ac3ca5d9e12fcce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='slice_view', options=('x', 'y', 'z'), value='x'), IntSlider(value=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function ipywidgets.widgets.interaction._InteractFactory.__call__.<locals>.<lambda>(*args, **kwargs)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# type(data_T1)\n",
    "slv1 = slice_view.slicer(data_ir)\n",
    "slv1.slicer_view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a7578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id =list(range(0, 5))\n",
    "patient_id\n",
    "\n",
    "import os\n",
    "root='/media/Data/MRI/datasets/'\n",
    "filename1 = '/media/Data/MRI/datasets/mrbrain_t1/cartesian/train/acc_4x/'\n",
    "filename2 = '/media/Data/MRI/datasets/mrbrain_ir/cartesian/train/acc_4x/'\n",
    "filename3 = '/media/Data/MRI/datasets/mrbrain_flair/cartesian/train/acc_4x/'\n",
    "if (os.path.isfile(os.path.join(filename1, str(2) + \".h5\"))):\n",
    "    print('yes')\n",
    "else: print('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afd19bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "root='/media/Data/MRI/datasets/'\n",
    "filename1 = '/media/Data/MRI/datasets/mrbrain_t1/cartesian/train/acc_4x/'\n",
    "filename2 = '/media/Data/MRI/datasets/mrbrain_ir/cartesian/train/acc_4x/'\n",
    "filename3 = '/media/Data/MRI/datasets/mrbrain_flair/cartesian/train/acc_4x/'\n",
    "\n",
    "class mrDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, root, mode=\"train\"):\n",
    "\n",
    "#         files = sorted(os.listdir(root))\n",
    "#         patient_id = list(set([i.split()[0] for i in files]))\n",
    "        patient_id =list(range(0, 5))\n",
    "\n",
    "        imgs_t1 = []\n",
    "        imgs_ir = []\n",
    "        imgs_flr = []\n",
    "\n",
    "        if mode == \"train\":\n",
    "            for i in range(len(patient_id)):\n",
    "                if (os.path.isfile(os.path.join(filename1, str(i) + \".h5\"))):\n",
    "                    imgs_t1.append(h5py.File(os.path.join(filename1, str(i) + \".h5\"), \"r\"))\n",
    "                if (os.path.isfile(os.path.join(filename2, str(i) + \".h5\"))):\n",
    "                    imgs_ir.append(h5py.File(os.path.join(filename2, str(i) + \".h5\"), \"r\"))\n",
    "                if (os.path.isfile(os.path.join(filename3, str(i) + \".h5\"))):\n",
    "                    imgs_flr.append(h5py.File(os.path.join(filename3, str(i) + \".h5\"), \"r\"))\n",
    "                    \n",
    "\n",
    "        elif mode == \"test\":\n",
    "            for i in range(2):\n",
    "                if (os.path.isfile(os.path.join(filename1, str(i) + \".h5\"))):\n",
    "                    imgs_t1.append(h5py.File(os.path.join(filename1, str(i) + \".h5\"), \"r\"))\n",
    "                if (os.path.isfile(os.path.join(filename2, str(i) + \".h5\"))):\n",
    "                    imgs_ir.append(h5py.File(os.path.join(filename2, str(i) + \".h5\"), \"r\"))\n",
    "                if (os.path.isfile(os.path.join(filename3, str(i) + \".h5\"))):\n",
    "                    imgs_flr.append(h5py.File(os.path.join(filename3, str(i) + \".h5\"), \"r\"))\n",
    "\n",
    "        self.imgs_t1 = imgs_t1\n",
    "        self.imgs_ir = imgs_ir\n",
    "        self.imgs_flr = imgs_flr\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "#         t1_path, ADC_path, PD_path, DCE_path = self.imgs[index]\n",
    "        \n",
    "        t1=self.imgs_t1[index]\n",
    "        a = list(t1.keys())[3]\n",
    "        data_t1 = np.asarray(t1[a])\n",
    "        \n",
    "        ir=self.imgs_ir[index]\n",
    "        b = list(ir.keys())[2]\n",
    "        data_ir = np.asarray(ir[b])\n",
    "        \n",
    "        flr=self.imgs_flr[index]\n",
    "        c = list(flr.keys())[2]\n",
    "        data_flr = np.asarray(flr[c])\n",
    "\n",
    "\n",
    "        return {\"T1\": data_t1, \"IR\": data_ir, \"FLR\":data_flr}\n",
    "#         return {\"data\": data,\"labels\": labels}\n",
    "#         return{}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbb82eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "print(f\"Using cuda device: {cuda}\")  # check if GPU is used\n",
    "\n",
    "# Tensor type (put everything on GPU if possible)\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff980816",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(mrDataset(root,mode='train'),batch_size=1,shuffle=False)\n",
    "\n",
    "\n",
    "# Parameters for Adam optimizer\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "\n",
    "# Create dataloaders\n",
    "# batch_size = 40\n",
    "train_loader =torch.utils.data.DataLoader(mrDataset(root,mode='train'),batch_size=1,shuffle=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(mrDataset(root,mode='test'),batch_size=1,shuffle=False)\n",
    "\n",
    "# Number of epochs\n",
    "num_epoch = 20\n",
    "\n",
    "# Train the generator\n",
    "# generator = train_generator(train_loader, test_loader, num_epoch=num_epoch,\n",
    "#                              lr=lr, beta1=beta1, beta2=beta2)\n",
    "\n",
    "for i, batch in enumerate(train_loader):\n",
    "\n",
    "    # Inputs T1-w and T2-w\n",
    "    real_t1 = batch[\"T1\"].type(Tensor)\n",
    "    print(real_t1.shape)\n",
    "    real_ir = batch[\"IR\"].type(Tensor)\n",
    "    print(real_ir.shape)\n",
    "    real_flr = batch[\"FLR\"].type(Tensor)\n",
    "    print(real_flr.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a480486",
   "metadata": {},
   "outputs": [],
   "source": [
    "root='/media/Data/MRI/datasets/'\n",
    "\n",
    "class SliceData(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset that provides access to MR image slices.\n",
    "    \"\"\"\n",
    "    filename1 = '/media/Data/MRI/datasets/mrbrain_t1/cartesian/train/acc_4x/'\n",
    "    filename2 = '/media/Data/MRI/datasets/mrbrain_ir/cartesian/train/acc_4x/'\n",
    "    filename3 = '/media/Data/MRI/datasets/mrbrain_flair/cartesian/train/acc_4x/'\n",
    "\n",
    "    #def __init__(self, root, acc_factor,dataset_type,mask_path): # acc_factor can be passed here and saved as self variable\n",
    "    def __init__(self, root): # acc_factor can be passed here and saved as self variable\n",
    "        # List the h5 files in root \n",
    "        fn1 = list(pathlib.Path(filename1).iterdir())\n",
    "        fn2 = list(pathlib.Path(filename2).iterdir())\n",
    "        fn3 = list(pathlib.Path(filename3).iterdir())\n",
    "        self.examples_t1 = []\n",
    "        self.examples_ir = []\n",
    "        self.examples_flr = []\n",
    "\n",
    "        for fname in sorted(fn1):\n",
    "            with h5py.File(fname,'r') as hf:\n",
    "                fsvol = hf['volfs']\n",
    "                num_slices = fsvol.shape[2]\n",
    "                self.examples_t1 += [(fname, slice) for slice in range(num_slices)]\n",
    "        for fname in sorted(fn2):\n",
    "            with h5py.File(fname,'r') as hf:\n",
    "                fsvol = hf['volfs']\n",
    "                num_slices = fsvol.shape[2]\n",
    "                self.examples_ir += [(fname, slice) for slice in range(num_slices)]\n",
    "        for fname in sorted(fn3):\n",
    "            with h5py.File(fname,'r') as hf:\n",
    "                fsvol = hf['volfs']\n",
    "                num_slices = fsvol.shape[2]\n",
    "                self.examples_flr += [(fname, slice) for slice in range(num_slices)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples_t1)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # Index the fname and slice using the list created in __init__\n",
    "        \n",
    "        fname1, slice = self.examples_t1[i] \n",
    "        fname2, slice = self.examples_ir[i]\n",
    "        fname3, slice = self.examples_flr[i]\n",
    "        # Print statements \n",
    "        #print (fname,slice)\n",
    "    \n",
    "        with h5py.File(fname1, 'r') as data:    \n",
    "            t1 = data['volfs'][:,:,slice].astype(np.float64)\n",
    "        with h5py.File(fname2, 'r') as data:    \n",
    "            ir = data['volfs'][:,:,slice].astype(np.float64)\n",
    "        with h5py.File(fname3, 'r') as data:    \n",
    "            flr = data['volfs'][:,:,slice].astype(np.float64)\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "        return torch.from_numpy(t1), torch.from_numpy(ir), torch.from_numpy(flr)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c07773c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filename1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\u001b[43mSliceData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m,batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36mSliceData.__init__\u001b[0;34m(self, root)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, root): \u001b[38;5;66;03m# acc_factor can be passed here and saved as self variable\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# List the h5 files in root \u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     fn1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(pathlib\u001b[38;5;241m.\u001b[39mPath(\u001b[43mfilename1\u001b[49m)\u001b[38;5;241m.\u001b[39miterdir())\n\u001b[1;32m     15\u001b[0m     fn2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(pathlib\u001b[38;5;241m.\u001b[39mPath(filename2)\u001b[38;5;241m.\u001b[39miterdir())\n\u001b[1;32m     16\u001b[0m     fn3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(pathlib\u001b[38;5;241m.\u001b[39mPath(filename3)\u001b[38;5;241m.\u001b[39miterdir())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filename1' is not defined"
     ]
    }
   ],
   "source": [
    "dataloader = torch.utils.data.DataLoader(SliceData(root),batch_size=1,shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca1a4401",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mdataloader\u001b[49m):\n\u001b[1;32m      2\u001b[0m     real_t1\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m     real_ir\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "for i,batch in enumerate(dataloader):\n",
    "    real_t1=batch[0]\n",
    "    real_ir=batch[1]\n",
    "    real_flr=batch[2]\n",
    "    print(real_t1.shape)\n",
    "    print(real_ir.shape)\n",
    "    print(real_flr.shape)\n",
    "    break\n",
    "real_t1.shape   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "528e8a4e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'real_t1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m d\u001b[38;5;241m=\u001b[39m\u001b[43mreal_t1\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m240\u001b[39m, \u001b[38;5;241m240\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# plt.imshow(d,'gray')\u001b[39;00m\n\u001b[1;32m      4\u001b[0m real_t1\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'real_t1' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "d=real_t1.reshape(240, 240)\n",
    "\n",
    "# plt.imshow(d,'gray')\n",
    "real_t1.size(0)\n",
    "# bts=imge2-im1.transpose()\n",
    "# d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c06a9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2=real_ir.reshape(240, 240)\n",
    "plt.imshow(d2,'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76dfcc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "d3=real_flr.reshape(240, 240)\n",
    "plt.imshow(d3,'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d196b9dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
