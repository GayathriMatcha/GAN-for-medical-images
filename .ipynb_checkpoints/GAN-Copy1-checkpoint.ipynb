{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qU5hIaEphitT"
   },
   "source": [
    "In this lab we will focus on **image synthesis**, in particular to\n",
    "synthesize **T2-weighted MRI** from **T1-weighted MRI**.\n",
    "\n",
    "We will investigate three approaches to do so:\n",
    "\n",
    "1. First, we will train a generator (or encoder-decoder).\n",
    "2. Then, we will train a conditional generative adversarial network (cGAN).\n",
    "3. Finally, we will train a cycle generative adversarial network (CycleGAN).\n",
    "\n",
    "We will evaluate the quality of the generated images using several metrics.\n",
    "\n",
    "We will use the [IXI dataset](https://brain-development.org/ixi-dataset/)\n",
    "to have access to **paired T1-w and T2-w images**.\n",
    "Before creating and training the different neural networks,\n",
    "we will:\n",
    "\n",
    "1. fetch the dataset,\n",
    "2. have a look at it to see what the task looks like, and\n",
    "3. illustrate how to easily access the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJtX86JuhitV"
   },
   "source": [
    "# Fetching the dataset:\n",
    "\n",
    "The dataset can be found on this\n",
    "[server](https://aramislab.paris.inria.fr/files/data/databases/DL4MI/IXI-dataset.tar.gz)\n",
    "and alternatively in the following\n",
    "[GitHub repository](https://github.com/Easternwen/IXI-dataset).\n",
    "In the `size64` folder, there are 1154 files: 2 images for 577 subjects.\n",
    "The size of each image is (64, 64).\n",
    "\n",
    "Let's download the file and have a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OC0rV0J4hitW"
   },
   "outputs": [],
   "source": [
    "# Get the dataset from the server\n",
    "#! git clone https://github.com/Easternwen/IXI-dataset.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Myw0XjChitX"
   },
   "source": [
    "The dataset used in this lab is composed of preprocessed images from the\n",
    "[IXI dataset](https://brain-development.org/ixi-dataset/). Two different\n",
    "structural MRI modalities are comprised in this dataset:\n",
    "\n",
    "- T1 weighted images\n",
    "\n",
    "- T2 weighted images\n",
    "\n",
    "These modalities do not highlight the same tissues: for example the CSF\n",
    "voxels are cancelled in T1 weighted imaging whereas they are highlighted by\n",
    "the T2 weighted imaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "N0q-Xo81hitY"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "root = \"./IXI-dataset/size64/\"\n",
    "\n",
    "plt.figure(figsize=(9, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(np.swapaxes(torch.load(os.path.join(root, 'sub-IXI002 - T1.pt')), 0, 1),\n",
    "           cmap='gray', origin='lower')\n",
    "plt.title(\"T1 slice for subject 002\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(np.swapaxes(torch.load(os.path.join(root, 'sub-IXI002 - T2.pt')), 0, 1),\n",
    "           cmap='gray', origin='lower')\n",
    "plt.title(\"T2 slice for subject 002\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPvcnZEChitZ"
   },
   "source": [
    "Let's import all the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5rhotxA-hitb",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# torch stuff\n",
    "#!pip install torchsummary\n",
    "#!pip install pandas\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# torchsummary and torchvision\n",
    "from torchsummary import summary\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# matplotlib stuff\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "\n",
    "# numpy and pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Common python packages\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mWuWmKozhitc"
   },
   "source": [
    "Let's create a custom `IXIDataset` class to easily have access to the data.\n",
    "Here we don't use tsv files to split subjects between the training and the\n",
    "test set. We only set the dataset to the `train` or `test` mode to access\n",
    "training or test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tAC_Pg4ehitd",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class IXIDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset utility class.\n",
    "\n",
    "    Args:\n",
    "        root: (str) Path of the folder with all the images.\n",
    "        mode : {'train' or 'test'} Part of the dataset that is loaded.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, root, mode=\"train\"):\n",
    "\n",
    "        files = sorted(os.listdir(root))\n",
    "        patient_id = list(set([i.split()[0] for i in files]))\n",
    "\n",
    "        imgs = []\n",
    "\n",
    "        if mode == \"train\":\n",
    "            for i in patient_id[:int(0.8 * len(patient_id))]:\n",
    "                if (\n",
    "                    os.path.isfile(os.path.join(root, i + \" - T1.pt\")) and\n",
    "                    os.path.isfile(os.path.join(root, i + \" - T2.pt\"))\n",
    "                ):\n",
    "                    imgs.append((os.path.join(root, i + \" - T1.pt\"),\n",
    "                                 os.path.join(root, i + \" - T2.pt\")))\n",
    "\n",
    "        elif mode == \"test\":\n",
    "            for i in patient_id[int(0.8 * len(patient_id)):]:\n",
    "                if (\n",
    "                    os.path.isfile(os.path.join(root, i + \" - T1.pt\")) and\n",
    "                    os.path.isfile(os.path.join(root, i + \" - T2.pt\"))\n",
    "                ):\n",
    "                    imgs.append((os.path.join(root, i + \" - T1.pt\"),\n",
    "                                 os.path.join(root, i + \" - T2.pt\")))\n",
    "\n",
    "        self.imgs = imgs\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        t1_path, t2_path = self.imgs[index]\n",
    "\n",
    "        t1 = torch.load(t1_path)[None, :, :]\n",
    "        t2 = torch.load(t2_path)[None, :, :]\n",
    "\n",
    "        return {\"T1\": t1, \"T2\": t2}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vu9tGhu3hite"
   },
   "source": [
    "Using this class and the `DataLoader` class from `torch.utils.data`, you can\n",
    "easily have access to your dataset. Here is a quick example on how to use it:\n",
    "\n",
    "```python\n",
    "# Create a DataLoader instance for the training set\n",
    "# You will get a batch of samples from the training set\n",
    "dataloader = DataLoader(\n",
    "    IXIDataset(root, mode=\"train\"),\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "for batch in dataloader:\n",
    "    # batch is a dictionary with two keys:\n",
    "    # - batch[\"T1\"] is a tensor with shape (batch_size, 64, 64) with the T1 images for the samples in this batch\n",
    "    # - batch[\"T2\"] is a tensor with shape (batch_size, 64, 64) with the T2 images for the samples in this batch\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=1\n",
    "dataloader = DataLoader(IXIDataset(root, mode=\"train\"), batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([1, 1, 64, 64])\n",
      "1\n",
      "torch.Size([1, 1, 64, 64])\n",
      "2\n",
      "torch.Size([1, 1, 64, 64])\n",
      "3\n",
      "torch.Size([1, 1, 64, 64])\n",
      "4\n",
      "torch.Size([1, 1, 64, 64])\n",
      "5\n",
      "torch.Size([1, 1, 64, 64])\n",
      "6\n",
      "torch.Size([1, 1, 64, 64])\n",
      "7\n",
      "torch.Size([1, 1, 64, 64])\n",
      "8\n",
      "torch.Size([1, 1, 64, 64])\n",
      "9\n",
      "torch.Size([1, 1, 64, 64])\n",
      "10\n",
      "torch.Size([1, 1, 64, 64])\n",
      "11\n",
      "torch.Size([1, 1, 64, 64])\n",
      "12\n",
      "torch.Size([1, 1, 64, 64])\n",
      "13\n",
      "torch.Size([1, 1, 64, 64])\n",
      "14\n",
      "torch.Size([1, 1, 64, 64])\n",
      "15\n",
      "torch.Size([1, 1, 64, 64])\n",
      "16\n",
      "torch.Size([1, 1, 64, 64])\n",
      "17\n",
      "torch.Size([1, 1, 64, 64])\n",
      "18\n",
      "torch.Size([1, 1, 64, 64])\n",
      "19\n",
      "torch.Size([1, 1, 64, 64])\n",
      "20\n",
      "torch.Size([1, 1, 64, 64])\n",
      "21\n",
      "torch.Size([1, 1, 64, 64])\n",
      "22\n",
      "torch.Size([1, 1, 64, 64])\n",
      "23\n",
      "torch.Size([1, 1, 64, 64])\n",
      "24\n",
      "torch.Size([1, 1, 64, 64])\n",
      "25\n",
      "torch.Size([1, 1, 64, 64])\n",
      "26\n",
      "torch.Size([1, 1, 64, 64])\n",
      "27\n",
      "torch.Size([1, 1, 64, 64])\n",
      "28\n",
      "torch.Size([1, 1, 64, 64])\n",
      "29\n",
      "torch.Size([1, 1, 64, 64])\n",
      "30\n",
      "torch.Size([1, 1, 64, 64])\n",
      "31\n",
      "torch.Size([1, 1, 64, 64])\n",
      "32\n",
      "torch.Size([1, 1, 64, 64])\n",
      "33\n",
      "torch.Size([1, 1, 64, 64])\n",
      "34\n",
      "torch.Size([1, 1, 64, 64])\n",
      "35\n",
      "torch.Size([1, 1, 64, 64])\n",
      "36\n",
      "torch.Size([1, 1, 64, 64])\n",
      "37\n",
      "torch.Size([1, 1, 64, 64])\n",
      "38\n",
      "torch.Size([1, 1, 64, 64])\n",
      "39\n",
      "torch.Size([1, 1, 64, 64])\n",
      "40\n",
      "torch.Size([1, 1, 64, 64])\n",
      "41\n",
      "torch.Size([1, 1, 64, 64])\n",
      "42\n",
      "torch.Size([1, 1, 64, 64])\n",
      "43\n",
      "torch.Size([1, 1, 64, 64])\n",
      "44\n",
      "torch.Size([1, 1, 64, 64])\n",
      "45\n",
      "torch.Size([1, 1, 64, 64])\n",
      "46\n",
      "torch.Size([1, 1, 64, 64])\n",
      "47\n",
      "torch.Size([1, 1, 64, 64])\n",
      "48\n",
      "torch.Size([1, 1, 64, 64])\n",
      "49\n",
      "torch.Size([1, 1, 64, 64])\n",
      "50\n",
      "torch.Size([1, 1, 64, 64])\n",
      "51\n",
      "torch.Size([1, 1, 64, 64])\n",
      "52\n",
      "torch.Size([1, 1, 64, 64])\n",
      "53\n",
      "torch.Size([1, 1, 64, 64])\n",
      "54\n",
      "torch.Size([1, 1, 64, 64])\n",
      "55\n",
      "torch.Size([1, 1, 64, 64])\n",
      "56\n",
      "torch.Size([1, 1, 64, 64])\n",
      "57\n",
      "torch.Size([1, 1, 64, 64])\n",
      "58\n",
      "torch.Size([1, 1, 64, 64])\n",
      "59\n",
      "torch.Size([1, 1, 64, 64])\n",
      "60\n",
      "torch.Size([1, 1, 64, 64])\n",
      "61\n",
      "torch.Size([1, 1, 64, 64])\n",
      "62\n",
      "torch.Size([1, 1, 64, 64])\n",
      "63\n",
      "torch.Size([1, 1, 64, 64])\n",
      "64\n",
      "torch.Size([1, 1, 64, 64])\n",
      "65\n",
      "torch.Size([1, 1, 64, 64])\n",
      "66\n",
      "torch.Size([1, 1, 64, 64])\n",
      "67\n",
      "torch.Size([1, 1, 64, 64])\n",
      "68\n",
      "torch.Size([1, 1, 64, 64])\n",
      "69\n",
      "torch.Size([1, 1, 64, 64])\n",
      "70\n",
      "torch.Size([1, 1, 64, 64])\n",
      "71\n",
      "torch.Size([1, 1, 64, 64])\n",
      "72\n",
      "torch.Size([1, 1, 64, 64])\n",
      "73\n",
      "torch.Size([1, 1, 64, 64])\n",
      "74\n",
      "torch.Size([1, 1, 64, 64])\n",
      "75\n",
      "torch.Size([1, 1, 64, 64])\n",
      "76\n",
      "torch.Size([1, 1, 64, 64])\n",
      "77\n",
      "torch.Size([1, 1, 64, 64])\n",
      "78\n",
      "torch.Size([1, 1, 64, 64])\n",
      "79\n",
      "torch.Size([1, 1, 64, 64])\n",
      "80\n",
      "torch.Size([1, 1, 64, 64])\n",
      "81\n",
      "torch.Size([1, 1, 64, 64])\n",
      "82\n",
      "torch.Size([1, 1, 64, 64])\n",
      "83\n",
      "torch.Size([1, 1, 64, 64])\n",
      "84\n",
      "torch.Size([1, 1, 64, 64])\n",
      "85\n",
      "torch.Size([1, 1, 64, 64])\n",
      "86\n",
      "torch.Size([1, 1, 64, 64])\n",
      "87\n",
      "torch.Size([1, 1, 64, 64])\n",
      "88\n",
      "torch.Size([1, 1, 64, 64])\n",
      "89\n",
      "torch.Size([1, 1, 64, 64])\n",
      "90\n",
      "torch.Size([1, 1, 64, 64])\n",
      "91\n",
      "torch.Size([1, 1, 64, 64])\n",
      "92\n",
      "torch.Size([1, 1, 64, 64])\n",
      "93\n",
      "torch.Size([1, 1, 64, 64])\n",
      "94\n",
      "torch.Size([1, 1, 64, 64])\n",
      "95\n",
      "torch.Size([1, 1, 64, 64])\n",
      "96\n",
      "torch.Size([1, 1, 64, 64])\n",
      "97\n",
      "torch.Size([1, 1, 64, 64])\n",
      "98\n",
      "torch.Size([1, 1, 64, 64])\n",
      "99\n",
      "torch.Size([1, 1, 64, 64])\n",
      "100\n",
      "torch.Size([1, 1, 64, 64])\n",
      "101\n",
      "torch.Size([1, 1, 64, 64])\n",
      "102\n",
      "torch.Size([1, 1, 64, 64])\n",
      "103\n",
      "torch.Size([1, 1, 64, 64])\n",
      "104\n",
      "torch.Size([1, 1, 64, 64])\n",
      "105\n",
      "torch.Size([1, 1, 64, 64])\n",
      "106\n",
      "torch.Size([1, 1, 64, 64])\n",
      "107\n",
      "torch.Size([1, 1, 64, 64])\n",
      "108\n",
      "torch.Size([1, 1, 64, 64])\n",
      "109\n",
      "torch.Size([1, 1, 64, 64])\n",
      "110\n",
      "torch.Size([1, 1, 64, 64])\n",
      "111\n",
      "torch.Size([1, 1, 64, 64])\n",
      "112\n",
      "torch.Size([1, 1, 64, 64])\n",
      "113\n",
      "torch.Size([1, 1, 64, 64])\n",
      "114\n",
      "torch.Size([1, 1, 64, 64])\n",
      "115\n",
      "torch.Size([1, 1, 64, 64])\n",
      "116\n",
      "torch.Size([1, 1, 64, 64])\n",
      "117\n",
      "torch.Size([1, 1, 64, 64])\n",
      "118\n",
      "torch.Size([1, 1, 64, 64])\n",
      "119\n",
      "torch.Size([1, 1, 64, 64])\n",
      "120\n",
      "torch.Size([1, 1, 64, 64])\n",
      "121\n",
      "torch.Size([1, 1, 64, 64])\n",
      "122\n",
      "torch.Size([1, 1, 64, 64])\n",
      "123\n",
      "torch.Size([1, 1, 64, 64])\n",
      "124\n",
      "torch.Size([1, 1, 64, 64])\n",
      "125\n",
      "torch.Size([1, 1, 64, 64])\n",
      "126\n",
      "torch.Size([1, 1, 64, 64])\n",
      "127\n",
      "torch.Size([1, 1, 64, 64])\n",
      "128\n",
      "torch.Size([1, 1, 64, 64])\n",
      "129\n",
      "torch.Size([1, 1, 64, 64])\n",
      "130\n",
      "torch.Size([1, 1, 64, 64])\n",
      "131\n",
      "torch.Size([1, 1, 64, 64])\n",
      "132\n",
      "torch.Size([1, 1, 64, 64])\n",
      "133\n",
      "torch.Size([1, 1, 64, 64])\n",
      "134\n",
      "torch.Size([1, 1, 64, 64])\n",
      "135\n",
      "torch.Size([1, 1, 64, 64])\n",
      "136\n",
      "torch.Size([1, 1, 64, 64])\n",
      "137\n",
      "torch.Size([1, 1, 64, 64])\n",
      "138\n",
      "torch.Size([1, 1, 64, 64])\n",
      "139\n",
      "torch.Size([1, 1, 64, 64])\n",
      "140\n",
      "torch.Size([1, 1, 64, 64])\n",
      "141\n",
      "torch.Size([1, 1, 64, 64])\n",
      "142\n",
      "torch.Size([1, 1, 64, 64])\n",
      "143\n",
      "torch.Size([1, 1, 64, 64])\n",
      "144\n",
      "torch.Size([1, 1, 64, 64])\n",
      "145\n",
      "torch.Size([1, 1, 64, 64])\n",
      "146\n",
      "torch.Size([1, 1, 64, 64])\n",
      "147\n",
      "torch.Size([1, 1, 64, 64])\n",
      "148\n",
      "torch.Size([1, 1, 64, 64])\n",
      "149\n",
      "torch.Size([1, 1, 64, 64])\n",
      "150\n",
      "torch.Size([1, 1, 64, 64])\n",
      "151\n",
      "torch.Size([1, 1, 64, 64])\n",
      "152\n",
      "torch.Size([1, 1, 64, 64])\n",
      "153\n",
      "torch.Size([1, 1, 64, 64])\n",
      "154\n",
      "torch.Size([1, 1, 64, 64])\n",
      "155\n",
      "torch.Size([1, 1, 64, 64])\n",
      "156\n",
      "torch.Size([1, 1, 64, 64])\n",
      "157\n",
      "torch.Size([1, 1, 64, 64])\n",
      "158\n",
      "torch.Size([1, 1, 64, 64])\n",
      "159\n",
      "torch.Size([1, 1, 64, 64])\n",
      "160\n",
      "torch.Size([1, 1, 64, 64])\n",
      "161\n",
      "torch.Size([1, 1, 64, 64])\n",
      "162\n",
      "torch.Size([1, 1, 64, 64])\n",
      "163\n",
      "torch.Size([1, 1, 64, 64])\n",
      "164\n",
      "torch.Size([1, 1, 64, 64])\n",
      "165\n",
      "torch.Size([1, 1, 64, 64])\n",
      "166\n",
      "torch.Size([1, 1, 64, 64])\n",
      "167\n",
      "torch.Size([1, 1, 64, 64])\n",
      "168\n",
      "torch.Size([1, 1, 64, 64])\n",
      "169\n",
      "torch.Size([1, 1, 64, 64])\n",
      "170\n",
      "torch.Size([1, 1, 64, 64])\n",
      "171\n",
      "torch.Size([1, 1, 64, 64])\n",
      "172\n",
      "torch.Size([1, 1, 64, 64])\n",
      "173\n",
      "torch.Size([1, 1, 64, 64])\n",
      "174\n",
      "torch.Size([1, 1, 64, 64])\n",
      "175\n",
      "torch.Size([1, 1, 64, 64])\n",
      "176\n",
      "torch.Size([1, 1, 64, 64])\n",
      "177\n",
      "torch.Size([1, 1, 64, 64])\n",
      "178\n",
      "torch.Size([1, 1, 64, 64])\n",
      "179\n",
      "torch.Size([1, 1, 64, 64])\n",
      "180\n",
      "torch.Size([1, 1, 64, 64])\n",
      "181\n",
      "torch.Size([1, 1, 64, 64])\n",
      "182\n",
      "torch.Size([1, 1, 64, 64])\n",
      "183\n",
      "torch.Size([1, 1, 64, 64])\n",
      "184\n",
      "torch.Size([1, 1, 64, 64])\n",
      "185\n",
      "torch.Size([1, 1, 64, 64])\n",
      "186\n",
      "torch.Size([1, 1, 64, 64])\n",
      "187\n",
      "torch.Size([1, 1, 64, 64])\n",
      "188\n",
      "torch.Size([1, 1, 64, 64])\n",
      "189\n",
      "torch.Size([1, 1, 64, 64])\n",
      "190\n",
      "torch.Size([1, 1, 64, 64])\n",
      "191\n",
      "torch.Size([1, 1, 64, 64])\n",
      "192\n",
      "torch.Size([1, 1, 64, 64])\n",
      "193\n",
      "torch.Size([1, 1, 64, 64])\n",
      "194\n",
      "torch.Size([1, 1, 64, 64])\n",
      "195\n",
      "torch.Size([1, 1, 64, 64])\n",
      "196\n",
      "torch.Size([1, 1, 64, 64])\n",
      "197\n",
      "torch.Size([1, 1, 64, 64])\n",
      "198\n",
      "torch.Size([1, 1, 64, 64])\n",
      "199\n",
      "torch.Size([1, 1, 64, 64])\n",
      "200\n",
      "torch.Size([1, 1, 64, 64])\n",
      "201\n",
      "torch.Size([1, 1, 64, 64])\n",
      "202\n",
      "torch.Size([1, 1, 64, 64])\n",
      "203\n",
      "torch.Size([1, 1, 64, 64])\n",
      "204\n",
      "torch.Size([1, 1, 64, 64])\n",
      "205\n",
      "torch.Size([1, 1, 64, 64])\n",
      "206\n",
      "torch.Size([1, 1, 64, 64])\n",
      "207\n",
      "torch.Size([1, 1, 64, 64])\n",
      "208\n",
      "torch.Size([1, 1, 64, 64])\n",
      "209\n",
      "torch.Size([1, 1, 64, 64])\n",
      "210\n",
      "torch.Size([1, 1, 64, 64])\n",
      "211\n",
      "torch.Size([1, 1, 64, 64])\n",
      "212\n",
      "torch.Size([1, 1, 64, 64])\n",
      "213\n",
      "torch.Size([1, 1, 64, 64])\n",
      "214\n",
      "torch.Size([1, 1, 64, 64])\n",
      "215\n",
      "torch.Size([1, 1, 64, 64])\n",
      "216\n",
      "torch.Size([1, 1, 64, 64])\n",
      "217\n",
      "torch.Size([1, 1, 64, 64])\n",
      "218\n",
      "torch.Size([1, 1, 64, 64])\n",
      "219\n",
      "torch.Size([1, 1, 64, 64])\n",
      "220\n",
      "torch.Size([1, 1, 64, 64])\n",
      "221\n",
      "torch.Size([1, 1, 64, 64])\n",
      "222\n",
      "torch.Size([1, 1, 64, 64])\n",
      "223\n",
      "torch.Size([1, 1, 64, 64])\n",
      "224\n",
      "torch.Size([1, 1, 64, 64])\n",
      "225\n",
      "torch.Size([1, 1, 64, 64])\n",
      "226\n",
      "torch.Size([1, 1, 64, 64])\n",
      "227\n",
      "torch.Size([1, 1, 64, 64])\n",
      "228\n",
      "torch.Size([1, 1, 64, 64])\n",
      "229\n",
      "torch.Size([1, 1, 64, 64])\n",
      "230\n",
      "torch.Size([1, 1, 64, 64])\n",
      "231\n",
      "torch.Size([1, 1, 64, 64])\n",
      "232\n",
      "torch.Size([1, 1, 64, 64])\n",
      "233\n",
      "torch.Size([1, 1, 64, 64])\n",
      "234\n",
      "torch.Size([1, 1, 64, 64])\n",
      "235\n",
      "torch.Size([1, 1, 64, 64])\n",
      "236\n",
      "torch.Size([1, 1, 64, 64])\n",
      "237\n",
      "torch.Size([1, 1, 64, 64])\n",
      "238\n",
      "torch.Size([1, 1, 64, 64])\n",
      "239\n",
      "torch.Size([1, 1, 64, 64])\n",
      "240\n",
      "torch.Size([1, 1, 64, 64])\n",
      "241\n",
      "torch.Size([1, 1, 64, 64])\n",
      "242\n",
      "torch.Size([1, 1, 64, 64])\n",
      "243\n",
      "torch.Size([1, 1, 64, 64])\n",
      "244\n",
      "torch.Size([1, 1, 64, 64])\n",
      "245\n",
      "torch.Size([1, 1, 64, 64])\n",
      "246\n",
      "torch.Size([1, 1, 64, 64])\n",
      "247\n",
      "torch.Size([1, 1, 64, 64])\n",
      "248\n",
      "torch.Size([1, 1, 64, 64])\n",
      "249\n",
      "torch.Size([1, 1, 64, 64])\n",
      "250\n",
      "torch.Size([1, 1, 64, 64])\n",
      "251\n",
      "torch.Size([1, 1, 64, 64])\n",
      "252\n",
      "torch.Size([1, 1, 64, 64])\n",
      "253\n",
      "torch.Size([1, 1, 64, 64])\n",
      "254\n",
      "torch.Size([1, 1, 64, 64])\n",
      "255\n",
      "torch.Size([1, 1, 64, 64])\n",
      "256\n",
      "torch.Size([1, 1, 64, 64])\n",
      "257\n",
      "torch.Size([1, 1, 64, 64])\n",
      "258\n",
      "torch.Size([1, 1, 64, 64])\n",
      "259\n",
      "torch.Size([1, 1, 64, 64])\n",
      "260\n",
      "torch.Size([1, 1, 64, 64])\n",
      "261\n",
      "torch.Size([1, 1, 64, 64])\n",
      "262\n",
      "torch.Size([1, 1, 64, 64])\n",
      "263\n",
      "torch.Size([1, 1, 64, 64])\n",
      "264\n",
      "torch.Size([1, 1, 64, 64])\n",
      "265\n",
      "torch.Size([1, 1, 64, 64])\n",
      "266\n",
      "torch.Size([1, 1, 64, 64])\n",
      "267\n",
      "torch.Size([1, 1, 64, 64])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268\n",
      "torch.Size([1, 1, 64, 64])\n",
      "269\n",
      "torch.Size([1, 1, 64, 64])\n",
      "270\n",
      "torch.Size([1, 1, 64, 64])\n",
      "271\n",
      "torch.Size([1, 1, 64, 64])\n",
      "272\n",
      "torch.Size([1, 1, 64, 64])\n",
      "273\n",
      "torch.Size([1, 1, 64, 64])\n",
      "274\n",
      "torch.Size([1, 1, 64, 64])\n",
      "275\n",
      "torch.Size([1, 1, 64, 64])\n",
      "276\n",
      "torch.Size([1, 1, 64, 64])\n",
      "277\n",
      "torch.Size([1, 1, 64, 64])\n",
      "278\n",
      "torch.Size([1, 1, 64, 64])\n",
      "279\n",
      "torch.Size([1, 1, 64, 64])\n",
      "280\n",
      "torch.Size([1, 1, 64, 64])\n",
      "281\n",
      "torch.Size([1, 1, 64, 64])\n",
      "282\n",
      "torch.Size([1, 1, 64, 64])\n",
      "283\n",
      "torch.Size([1, 1, 64, 64])\n",
      "284\n",
      "torch.Size([1, 1, 64, 64])\n",
      "285\n",
      "torch.Size([1, 1, 64, 64])\n",
      "286\n",
      "torch.Size([1, 1, 64, 64])\n",
      "287\n",
      "torch.Size([1, 1, 64, 64])\n",
      "288\n",
      "torch.Size([1, 1, 64, 64])\n",
      "289\n",
      "torch.Size([1, 1, 64, 64])\n",
      "290\n",
      "torch.Size([1, 1, 64, 64])\n",
      "291\n",
      "torch.Size([1, 1, 64, 64])\n",
      "292\n",
      "torch.Size([1, 1, 64, 64])\n",
      "293\n",
      "torch.Size([1, 1, 64, 64])\n",
      "294\n",
      "torch.Size([1, 1, 64, 64])\n",
      "295\n",
      "torch.Size([1, 1, 64, 64])\n",
      "296\n",
      "torch.Size([1, 1, 64, 64])\n",
      "297\n",
      "torch.Size([1, 1, 64, 64])\n",
      "298\n",
      "torch.Size([1, 1, 64, 64])\n",
      "299\n",
      "torch.Size([1, 1, 64, 64])\n",
      "300\n",
      "torch.Size([1, 1, 64, 64])\n",
      "301\n",
      "torch.Size([1, 1, 64, 64])\n",
      "302\n",
      "torch.Size([1, 1, 64, 64])\n",
      "303\n",
      "torch.Size([1, 1, 64, 64])\n",
      "304\n",
      "torch.Size([1, 1, 64, 64])\n",
      "305\n",
      "torch.Size([1, 1, 64, 64])\n",
      "306\n",
      "torch.Size([1, 1, 64, 64])\n",
      "307\n",
      "torch.Size([1, 1, 64, 64])\n",
      "308\n",
      "torch.Size([1, 1, 64, 64])\n",
      "309\n",
      "torch.Size([1, 1, 64, 64])\n",
      "310\n",
      "torch.Size([1, 1, 64, 64])\n",
      "311\n",
      "torch.Size([1, 1, 64, 64])\n",
      "312\n",
      "torch.Size([1, 1, 64, 64])\n",
      "313\n",
      "torch.Size([1, 1, 64, 64])\n",
      "314\n",
      "torch.Size([1, 1, 64, 64])\n",
      "315\n",
      "torch.Size([1, 1, 64, 64])\n",
      "316\n",
      "torch.Size([1, 1, 64, 64])\n",
      "317\n",
      "torch.Size([1, 1, 64, 64])\n",
      "318\n",
      "torch.Size([1, 1, 64, 64])\n",
      "319\n",
      "torch.Size([1, 1, 64, 64])\n",
      "320\n",
      "torch.Size([1, 1, 64, 64])\n",
      "321\n",
      "torch.Size([1, 1, 64, 64])\n",
      "322\n",
      "torch.Size([1, 1, 64, 64])\n",
      "323\n",
      "torch.Size([1, 1, 64, 64])\n",
      "324\n",
      "torch.Size([1, 1, 64, 64])\n",
      "325\n",
      "torch.Size([1, 1, 64, 64])\n",
      "326\n",
      "torch.Size([1, 1, 64, 64])\n",
      "327\n",
      "torch.Size([1, 1, 64, 64])\n",
      "328\n",
      "torch.Size([1, 1, 64, 64])\n",
      "329\n",
      "torch.Size([1, 1, 64, 64])\n",
      "330\n",
      "torch.Size([1, 1, 64, 64])\n",
      "331\n",
      "torch.Size([1, 1, 64, 64])\n",
      "332\n",
      "torch.Size([1, 1, 64, 64])\n",
      "333\n",
      "torch.Size([1, 1, 64, 64])\n",
      "334\n",
      "torch.Size([1, 1, 64, 64])\n",
      "335\n",
      "torch.Size([1, 1, 64, 64])\n",
      "336\n",
      "torch.Size([1, 1, 64, 64])\n",
      "337\n",
      "torch.Size([1, 1, 64, 64])\n",
      "338\n",
      "torch.Size([1, 1, 64, 64])\n",
      "339\n",
      "torch.Size([1, 1, 64, 64])\n",
      "340\n",
      "torch.Size([1, 1, 64, 64])\n",
      "341\n",
      "torch.Size([1, 1, 64, 64])\n",
      "342\n",
      "torch.Size([1, 1, 64, 64])\n",
      "343\n",
      "torch.Size([1, 1, 64, 64])\n",
      "344\n",
      "torch.Size([1, 1, 64, 64])\n",
      "345\n",
      "torch.Size([1, 1, 64, 64])\n",
      "346\n",
      "torch.Size([1, 1, 64, 64])\n",
      "347\n",
      "torch.Size([1, 1, 64, 64])\n",
      "348\n",
      "torch.Size([1, 1, 64, 64])\n",
      "349\n",
      "torch.Size([1, 1, 64, 64])\n",
      "350\n",
      "torch.Size([1, 1, 64, 64])\n",
      "351\n",
      "torch.Size([1, 1, 64, 64])\n",
      "352\n",
      "torch.Size([1, 1, 64, 64])\n",
      "353\n",
      "torch.Size([1, 1, 64, 64])\n",
      "354\n",
      "torch.Size([1, 1, 64, 64])\n",
      "355\n",
      "torch.Size([1, 1, 64, 64])\n",
      "356\n",
      "torch.Size([1, 1, 64, 64])\n",
      "357\n",
      "torch.Size([1, 1, 64, 64])\n",
      "358\n",
      "torch.Size([1, 1, 64, 64])\n",
      "359\n",
      "torch.Size([1, 1, 64, 64])\n",
      "360\n",
      "torch.Size([1, 1, 64, 64])\n",
      "361\n",
      "torch.Size([1, 1, 64, 64])\n",
      "362\n",
      "torch.Size([1, 1, 64, 64])\n",
      "363\n",
      "torch.Size([1, 1, 64, 64])\n",
      "364\n",
      "torch.Size([1, 1, 64, 64])\n",
      "365\n",
      "torch.Size([1, 1, 64, 64])\n",
      "366\n",
      "torch.Size([1, 1, 64, 64])\n",
      "367\n",
      "torch.Size([1, 1, 64, 64])\n",
      "368\n",
      "torch.Size([1, 1, 64, 64])\n",
      "369\n",
      "torch.Size([1, 1, 64, 64])\n",
      "370\n",
      "torch.Size([1, 1, 64, 64])\n",
      "371\n",
      "torch.Size([1, 1, 64, 64])\n",
      "372\n",
      "torch.Size([1, 1, 64, 64])\n",
      "373\n",
      "torch.Size([1, 1, 64, 64])\n",
      "374\n",
      "torch.Size([1, 1, 64, 64])\n",
      "375\n",
      "torch.Size([1, 1, 64, 64])\n",
      "376\n",
      "torch.Size([1, 1, 64, 64])\n",
      "377\n",
      "torch.Size([1, 1, 64, 64])\n",
      "378\n",
      "torch.Size([1, 1, 64, 64])\n",
      "379\n",
      "torch.Size([1, 1, 64, 64])\n",
      "380\n",
      "torch.Size([1, 1, 64, 64])\n",
      "381\n",
      "torch.Size([1, 1, 64, 64])\n",
      "382\n",
      "torch.Size([1, 1, 64, 64])\n",
      "383\n",
      "torch.Size([1, 1, 64, 64])\n",
      "384\n",
      "torch.Size([1, 1, 64, 64])\n",
      "385\n",
      "torch.Size([1, 1, 64, 64])\n",
      "386\n",
      "torch.Size([1, 1, 64, 64])\n",
      "387\n",
      "torch.Size([1, 1, 64, 64])\n",
      "388\n",
      "torch.Size([1, 1, 64, 64])\n",
      "389\n",
      "torch.Size([1, 1, 64, 64])\n",
      "390\n",
      "torch.Size([1, 1, 64, 64])\n",
      "391\n",
      "torch.Size([1, 1, 64, 64])\n",
      "392\n",
      "torch.Size([1, 1, 64, 64])\n",
      "393\n",
      "torch.Size([1, 1, 64, 64])\n",
      "394\n",
      "torch.Size([1, 1, 64, 64])\n",
      "395\n",
      "torch.Size([1, 1, 64, 64])\n",
      "396\n",
      "torch.Size([1, 1, 64, 64])\n",
      "397\n",
      "torch.Size([1, 1, 64, 64])\n",
      "398\n",
      "torch.Size([1, 1, 64, 64])\n",
      "399\n",
      "torch.Size([1, 1, 64, 64])\n",
      "400\n",
      "torch.Size([1, 1, 64, 64])\n",
      "401\n",
      "torch.Size([1, 1, 64, 64])\n",
      "402\n",
      "torch.Size([1, 1, 64, 64])\n",
      "403\n",
      "torch.Size([1, 1, 64, 64])\n",
      "404\n",
      "torch.Size([1, 1, 64, 64])\n",
      "405\n",
      "torch.Size([1, 1, 64, 64])\n",
      "406\n",
      "torch.Size([1, 1, 64, 64])\n",
      "407\n",
      "torch.Size([1, 1, 64, 64])\n",
      "408\n",
      "torch.Size([1, 1, 64, 64])\n",
      "409\n",
      "torch.Size([1, 1, 64, 64])\n",
      "410\n",
      "torch.Size([1, 1, 64, 64])\n",
      "411\n",
      "torch.Size([1, 1, 64, 64])\n",
      "412\n",
      "torch.Size([1, 1, 64, 64])\n",
      "413\n",
      "torch.Size([1, 1, 64, 64])\n",
      "414\n",
      "torch.Size([1, 1, 64, 64])\n",
      "415\n",
      "torch.Size([1, 1, 64, 64])\n",
      "416\n",
      "torch.Size([1, 1, 64, 64])\n",
      "417\n",
      "torch.Size([1, 1, 64, 64])\n",
      "418\n",
      "torch.Size([1, 1, 64, 64])\n",
      "419\n",
      "torch.Size([1, 1, 64, 64])\n",
      "420\n",
      "torch.Size([1, 1, 64, 64])\n",
      "421\n",
      "torch.Size([1, 1, 64, 64])\n",
      "422\n",
      "torch.Size([1, 1, 64, 64])\n",
      "423\n",
      "torch.Size([1, 1, 64, 64])\n",
      "424\n",
      "torch.Size([1, 1, 64, 64])\n",
      "425\n",
      "torch.Size([1, 1, 64, 64])\n",
      "426\n",
      "torch.Size([1, 1, 64, 64])\n",
      "427\n",
      "torch.Size([1, 1, 64, 64])\n",
      "428\n",
      "torch.Size([1, 1, 64, 64])\n",
      "429\n",
      "torch.Size([1, 1, 64, 64])\n",
      "430\n",
      "torch.Size([1, 1, 64, 64])\n",
      "431\n",
      "torch.Size([1, 1, 64, 64])\n",
      "432\n",
      "torch.Size([1, 1, 64, 64])\n",
      "433\n",
      "torch.Size([1, 1, 64, 64])\n",
      "434\n",
      "torch.Size([1, 1, 64, 64])\n",
      "435\n",
      "torch.Size([1, 1, 64, 64])\n",
      "436\n",
      "torch.Size([1, 1, 64, 64])\n",
      "437\n",
      "torch.Size([1, 1, 64, 64])\n",
      "438\n",
      "torch.Size([1, 1, 64, 64])\n",
      "439\n",
      "torch.Size([1, 1, 64, 64])\n",
      "440\n",
      "torch.Size([1, 1, 64, 64])\n",
      "441\n",
      "torch.Size([1, 1, 64, 64])\n",
      "442\n",
      "torch.Size([1, 1, 64, 64])\n",
      "443\n",
      "torch.Size([1, 1, 64, 64])\n",
      "444\n",
      "torch.Size([1, 1, 64, 64])\n",
      "445\n",
      "torch.Size([1, 1, 64, 64])\n",
      "446\n",
      "torch.Size([1, 1, 64, 64])\n",
      "447\n",
      "torch.Size([1, 1, 64, 64])\n",
      "448\n",
      "torch.Size([1, 1, 64, 64])\n",
      "449\n",
      "torch.Size([1, 1, 64, 64])\n",
      "450\n",
      "torch.Size([1, 1, 64, 64])\n",
      "451\n",
      "torch.Size([1, 1, 64, 64])\n",
      "452\n",
      "torch.Size([1, 1, 64, 64])\n",
      "453\n",
      "torch.Size([1, 1, 64, 64])\n",
      "454\n",
      "torch.Size([1, 1, 64, 64])\n",
      "455\n",
      "torch.Size([1, 1, 64, 64])\n",
      "456\n",
      "torch.Size([1, 1, 64, 64])\n",
      "457\n",
      "torch.Size([1, 1, 64, 64])\n",
      "458\n",
      "torch.Size([1, 1, 64, 64])\n",
      "459\n",
      "torch.Size([1, 1, 64, 64])\n",
      "460\n",
      "torch.Size([1, 1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i,batch in enumerate(dataloader):\n",
    "    print(i)\n",
    "    print(batch['T2'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tuk_qnDyhite"
   },
   "source": [
    "# 1. Generator\n",
    "\n",
    "## 1.1 Architecture\n",
    "\n",
    "The generator will have a **U-Net architecture** with the following\n",
    "characteristics:\n",
    "\n",
    "* the descending blocks are convolutional layers followed by instance\n",
    "  normalization with a LeakyReLU activation function;\n",
    "\n",
    "* the ascending blocks are transposed convolutional layers followed by\n",
    "  instance normalization with a ReLU activation function.\n",
    "\n",
    "The parameters for each layer are given in the picture below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I7_RHHsghitf"
   },
   "source": [
    "<a href=\"https://ibb.co/QXBDNy3\">\n",
    "    <img src=\"https://i.ibb.co/g614TkL/Capture-d-cran-2020-03-02-16-04-06.png\" width=\"800\" border=\"0\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aKPacMbhitg"
   },
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Exercise</b>: Create a <code>GeneratorUNet</code> class to define the\n",
    "generator with the architecture given above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "pRNz8a0qhitg",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# We provide classes for each block of the U-Net.\n",
    "\n",
    "class UNetDown(nn.Module):\n",
    "\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(UNetDown, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_size, out_size, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(out_size),\n",
    "            nn.LeakyReLU(0.2)\n",
    "          )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class UNetUp(nn.Module):\n",
    "\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(UNetUp, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_size, out_size, kernel_size=4,\n",
    "                               stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(out_size),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip_input=None):\n",
    "        if skip_input is not None:\n",
    "            x = torch.cat((x, skip_input), 1)  # add the skip connection\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FinalLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(FinalLayer, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(in_size, out_size, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip_input=None):\n",
    "        if skip_input is not None:\n",
    "            x = torch.cat((x, skip_input), 1)  # add the skip connection\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9iucCD-_hith"
   },
   "outputs": [],
   "source": [
    "class GeneratorUNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1):\n",
    "        super(GeneratorUNet, self).__init__()\n",
    "        # To complete\n",
    "        features_g=64\n",
    "\n",
    "        self.und1=UNetDown(in_channels, features_g) #img 64*32*32\n",
    "        self.und2=UNetDown(features_g, features_g*2) #img 128*16*16\n",
    "        self.und3=UNetDown(features_g*2, features_g*4) #img 256*8*8\n",
    "        self.und4=UNetDown(features_g*4, features_g*8) #img 512*4*4 \n",
    "        \n",
    "        self.mid=nn.Sequential(\n",
    "            nn.Conv2d(features_g*8, features_g*8, kernel_size=3, stride=2, padding=1), #img 2x2\n",
    "            nn.InstanceNorm2d(features_g*8),\n",
    "            nn.ReLU(inplace=True),        \n",
    "        )#512*2*2\n",
    "        \n",
    "        self.unup_1 = UNetUp(features_g*8, features_g*8)#img 512*4*4\n",
    "        self.unup_2 = UNetUp(features_g*8*2, features_g*4) #img 256*8*8\n",
    "        self.unup_3 = UNetUp(features_g*4*2, features_g*2) #img 128*16*16\n",
    "        self.unup_4 = UNetUp(features_g*2*2, features_g) #img 64*32*32\n",
    "        self.final_layer = FinalLayer(features_g*2,1)#1*64*64\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.und1(x)\n",
    "        x2 = self.und2(x1)\n",
    "        x3 = self.und3(x2)\n",
    "        x4 = self.und4(x3)\n",
    "        x = self.mid(x4)\n",
    "        x = self.unup_1(x)\n",
    "        x = self.unup_2(x,skip_input=x4)\n",
    "        x = self.unup_3(x,skip_input=x3)\n",
    "        x = self.unup_4(x,skip_input=x2)\n",
    "        x = self.final_layer(x,skip_input=x1)      \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7XJZBO4hith"
   },
   "source": [
    "Let's have a look at the architecture of our generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "aEDdDVAhhith",
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 32, 32]             640\n",
      "    InstanceNorm2d-2           [-1, 64, 32, 32]               0\n",
      "         LeakyReLU-3           [-1, 64, 32, 32]               0\n",
      "          UNetDown-4           [-1, 64, 32, 32]               0\n",
      "            Conv2d-5          [-1, 128, 16, 16]          73,856\n",
      "    InstanceNorm2d-6          [-1, 128, 16, 16]               0\n",
      "         LeakyReLU-7          [-1, 128, 16, 16]               0\n",
      "          UNetDown-8          [-1, 128, 16, 16]               0\n",
      "            Conv2d-9            [-1, 256, 8, 8]         295,168\n",
      "   InstanceNorm2d-10            [-1, 256, 8, 8]               0\n",
      "        LeakyReLU-11            [-1, 256, 8, 8]               0\n",
      "         UNetDown-12            [-1, 256, 8, 8]               0\n",
      "           Conv2d-13            [-1, 512, 4, 4]       1,180,160\n",
      "   InstanceNorm2d-14            [-1, 512, 4, 4]               0\n",
      "        LeakyReLU-15            [-1, 512, 4, 4]               0\n",
      "         UNetDown-16            [-1, 512, 4, 4]               0\n",
      "           Conv2d-17            [-1, 512, 2, 2]       2,359,808\n",
      "   InstanceNorm2d-18            [-1, 512, 2, 2]               0\n",
      "             ReLU-19            [-1, 512, 2, 2]               0\n",
      "  ConvTranspose2d-20            [-1, 512, 4, 4]       4,194,816\n",
      "   InstanceNorm2d-21            [-1, 512, 4, 4]               0\n",
      "             ReLU-22            [-1, 512, 4, 4]               0\n",
      "           UNetUp-23            [-1, 512, 4, 4]               0\n",
      "  ConvTranspose2d-24            [-1, 256, 8, 8]       4,194,560\n",
      "   InstanceNorm2d-25            [-1, 256, 8, 8]               0\n",
      "             ReLU-26            [-1, 256, 8, 8]               0\n",
      "           UNetUp-27            [-1, 256, 8, 8]               0\n",
      "  ConvTranspose2d-28          [-1, 128, 16, 16]       1,048,704\n",
      "   InstanceNorm2d-29          [-1, 128, 16, 16]               0\n",
      "             ReLU-30          [-1, 128, 16, 16]               0\n",
      "           UNetUp-31          [-1, 128, 16, 16]               0\n",
      "  ConvTranspose2d-32           [-1, 64, 32, 32]         262,208\n",
      "   InstanceNorm2d-33           [-1, 64, 32, 32]               0\n",
      "             ReLU-34           [-1, 64, 32, 32]               0\n",
      "           UNetUp-35           [-1, 64, 32, 32]               0\n",
      "         Upsample-36          [-1, 128, 64, 64]               0\n",
      "           Conv2d-37            [-1, 1, 64, 64]           1,153\n",
      "             Tanh-38            [-1, 1, 64, 64]               0\n",
      "       FinalLayer-39            [-1, 1, 64, 64]               0\n",
      "================================================================\n",
      "Total params: 13,611,073\n",
      "Trainable params: 13,611,073\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.02\n",
      "Forward/backward pass size (MB): 11.64\n",
      "Params size (MB): 51.92\n",
      "Estimated Total Size (MB): 63.58\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Summary of the generator\n",
    "summary(GeneratorUNet().cuda(), (1, 64, 64) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gwm-huy-hiti"
   },
   "source": [
    "## 1.2 Train the generator\n",
    "\n",
    "In order to train the generator, we will repeat the following process:\n",
    "\n",
    "1. Generate T2-w images from T1-w images.\n",
    "2. Compute the error between the true T2-w images and the generated T2-w images.\n",
    "3. Update the parameters of the generators.\n",
    "\n",
    "The training phase looks like this:\n",
    "\n",
    "```\n",
    "# For each epoch\n",
    "\n",
    "    # For each batch\n",
    "\n",
    "        # Generate fake images for all the images in this batch\n",
    "\n",
    "        # Compute the loss for the generator\n",
    "\n",
    "        # Perform one optimization step\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = torch.randn(4, 4)\n",
    "# m=torch.cat((t,t,t),-2)\n",
    "# m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umaysuSOhiti"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Exercise</b>: We provide below a template to train our generator\n",
    " on the dataset. Fill in the missing parts.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lknXe9jEhitj",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_generator(train_loader, test_loader, num_epoch=500,\n",
    "                    lr=0.0001, beta1=0.9, beta2=0.999):\n",
    "    \"\"\"Train a generator on its own.\n",
    "\n",
    "    Args:\n",
    "        train_loader: (DataLoader) a DataLoader wrapping the training dataset\n",
    "        test_loader: (DataLoader) a DataLoader wrapping the test dataset\n",
    "        num_epoch: (int) number of epochs performed during training\n",
    "        lr: (float) learning rate of the discriminator and generator Adam optimizers\n",
    "        beta1: (float) beta1 coefficient of the discriminator and generator Adam optimizers\n",
    "        beta2: (float) beta1 coefficient of the discriminator and generator Adam optimizers\n",
    "\n",
    "    Returns:\n",
    "        generator: (nn.Module) the trained generator\n",
    "    \"\"\"\n",
    "\n",
    "    cuda = True if torch.cuda.is_available() else False\n",
    "    print(f\"Using cuda device: {cuda}\")  # check if GPU is used\n",
    "\n",
    "    # Tensor type (put everything on GPU if possible)\n",
    "    Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "    # Output folder\n",
    "    if not os.path.exists(\"./images/generator\"):\n",
    "        os.makedirs(\"./images/generator\")\n",
    "\n",
    "    # Loss function\n",
    "    criterion =  nn.L1Loss()  # To complete. A loss for a voxel-wise comparison of images like torch.nn.L1Loss\n",
    "\n",
    "    # Initialize the generator\n",
    "#    generator = GeneratorUnet(in_channels=1, out_channels=1)\n",
    "#    generator = GeneratorUNet().cuda()\n",
    "    generator = GeneratorUNet(1,1)\n",
    "\n",
    "    if cuda:\n",
    "        generator = generator.cuda()\n",
    "        criterion.cuda()\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(generator.parameters(),\n",
    "                                 lr=lr, betas=(beta1, beta2))\n",
    "\n",
    "    def sample_images(epoch):\n",
    "        \"\"\"Saves a generated sample from the validation set\"\"\"\n",
    "        imgs = next(iter(test_loader))\n",
    "        real_A = imgs[\"T1\"].type(Tensor)\n",
    "        real_B = imgs[\"T2\"].type(Tensor)\n",
    "        fake_B = generator(real_A)\n",
    "        img_sample = torch.cat((real_A.data, fake_B.data, real_B.data), -2)\n",
    "        save_image(img_sample, f\"./images/generator/epoch-{epoch}.png\",nrow=5, normalize=True)\n",
    "        \n",
    "\n",
    "    # ----------\n",
    "    #  Training\n",
    "    # ----------\n",
    "\n",
    "    prev_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        for i, batch in enumerate(train_loader):\n",
    "\n",
    "            # Inputs T1-w and T2-w\n",
    "            print(batch['T2'].shape)\n",
    "            real_t1 = batch[\"T1\"].type(Tensor)\n",
    "            real_t2 = batch[\"T2\"].type(Tensor)\n",
    "\n",
    "            # Remove stored gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Generate fake T2 images from the true T1 images\n",
    "#             fake_t2 =    # To complete\n",
    "            fake_t2= generator(real_t1)\n",
    "    \n",
    "\n",
    "            # Compute the corresponding loss\n",
    "#            loss =     # To complete\n",
    "            loss = criterion(fake_t2,real_t2)   # To complete\n",
    "\n",
    "            # Compute the gradient and perform one optimization step\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # --------------\n",
    "            #  Log Progress\n",
    "            # --------------\n",
    "\n",
    "            # Determine approximate time left\n",
    "            batches_done = epoch * len(train_loader) + i\n",
    "            batches_left = num_epoch * len(train_loader) - batches_done\n",
    "            time_left = datetime.timedelta(\n",
    "                seconds=batches_left * (time.time() - prev_time))\n",
    "            prev_time = time.time()\n",
    "\n",
    "            # Print log\n",
    "            sys.stdout.write(\n",
    "                \"\\r[Epoch %d/%d] [Batch %d/%d] [Loss: %f] ETA: %s\"\n",
    "                % (\n",
    "                    epoch + 1,\n",
    "                    num_epoch,\n",
    "                    i,\n",
    "                    len(train_loader),\n",
    "                    loss.item(),\n",
    "                    time_left,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Save images at the end of each epoch\n",
    "        sample_images(epoch)\n",
    "\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jOUQ6tvlhitj",
    "tags": [
     "remove_output"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters for Adam optimizer\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 40\n",
    "train_loader = DataLoader(IXIDataset(root, mode=\"train\"),\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(IXIDataset(root, mode=\"test\"),\n",
    "                         batch_size=5,\n",
    "                         shuffle=False)\n",
    "\n",
    "# Number of epochs\n",
    "num_epoch = 20\n",
    "\n",
    "# Train the generator\n",
    "# generator = train_generator(train_loader, test_loader, num_epoch=num_epoch,\n",
    "#                              lr=lr, beta1=beta1, beta2=beta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TmFQfH2Vhitk"
   },
   "source": [
    "## 1.3 Evaluate the generator\n",
    "\n",
    "Let's visualize a few generated T2-weighted images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4QndUwSohitk",
    "lines_to_next_cell": 1,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "im = img.imread(f'./images/generator/epoch-{num_epoch - 1}.png')\n",
    "plt.figure(figsize=(20, 20))\n",
    "plt.imshow(np.swapaxes(im, 0, 1))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wym1ab47hitl"
   },
   "source": [
    "After doing visual quality control, it is a good idea to quantify the quality\n",
    "of the generated images using specific metrics. Some popular metrics include\n",
    "the Mean Absolute Error (MAE), the Peak Signal-to-Noise Ratio (PSNR) and\n",
    "the Structural Similarity index (SSIM):\n",
    "\n",
    "* MAE = $\\displaystyle \\frac{1}{nm} \\sum_{i=1}^n \\sum_{j=1}^m \\vert T_{ij} - G_{ij} \\vert $\n",
    "\n",
    "* PSNR = $\\displaystyle -10 \\log_{10} \\left( \\frac{1}{nm} \\sum_{i=1}^n \\sum_{j=1}^m (T_{ij} - G_{ij})^2 \\right) $\n",
    "\n",
    "* SSIM = $\\displaystyle  \\frac{(2 \\mu_T \\mu_G + C_1)(2 \\sigma_{TG} + C_2)}{(\\mu_T^2 +\n",
    "\\mu_G^2 + C_1)(\\sigma_T^2 + \\sigma_G^2 + C_2)} $ where:\n",
    "\n",
    "    * $\\mu$ and $\\sigma$ are the mean value and standard deviation of an image respectively, and\n",
    "    * $C_1$ and $C_2$ are two positive constants (one can take $C_1=0.01$ and $C_2=0.03$).\n",
    "\n",
    "The [mean absolute error](https://en.wikipedia.org/wiki/Mean_absolute_error)\n",
    "is simply the mean of each absolute value of the difference between\n",
    "the true pixel ($T_{ij}$) and the generated pixel ($G_{ij}$).\n",
    "The lower, the better. Minimum value is 0.\n",
    "\n",
    "The [peak signal-to-noise ratio](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio)\n",
    "is a function of the mean squared error and allows for comparing\n",
    "images encoded with different scales. We simplified its formula in our case.\n",
    "The higher, the better. Maximum value is $+\\infty$.\n",
    "\n",
    "The [structural similarity index](https://en.wikipedia.org/wiki/Structural_similarity)\n",
    "is a weighted combination of the luminance, the contrast and the structure.\n",
    "The higher, the better. Maximum value is 1.\n",
    "\n",
    "For those interested, you can find [here](https://www.pyimagesearch.com/2014/09/15/python-compare-two-images/)\n",
    "a reference to better understand the differences between these metrics.\n",
    "\n",
    "We provide an implementation for each metric with the functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ACCxTmEhitl",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def mean_absolute_error(image_true, image_generated):\n",
    "    \"\"\"Compute mean absolute error.\n",
    "\n",
    "    Args:\n",
    "        image_true: (Tensor) true image\n",
    "        image_generated: (Tensor) generated image\n",
    "\n",
    "    Returns:\n",
    "        mse: (float) mean squared error\n",
    "    \"\"\"\n",
    "    return torch.abs(image_true - image_generated).mean()\n",
    "\n",
    "\n",
    "def peak_signal_to_noise_ratio(image_true, image_generated):\n",
    "    \"\"\"\"Compute peak signal-to-noise ratio.\n",
    "\n",
    "    Args:\n",
    "        image_true: (Tensor) true image\n",
    "        image_generated: (Tensor) generated image\n",
    "\n",
    "    Returns:\n",
    "        psnr: (float) peak signal-to-noise ratio\"\"\"\n",
    "    mse = ((image_true - image_generated) ** 2).mean().cpu()\n",
    "    return -10 * np.log10(mse)\n",
    "\n",
    "\n",
    "def structural_similarity_index(image_true, image_generated, C1=0.01, C2=0.03):\n",
    "    \"\"\"Compute structural similarity index.\n",
    "\n",
    "    Args:\n",
    "        image_true: (Tensor) true image\n",
    "        image_generated: (Tensor) generated image\n",
    "        C1: (float) variable to stabilize the denominator\n",
    "        C2: (float) variable to stabilize the denominator\n",
    "\n",
    "    Returns:\n",
    "        ssim: (float) mean squared error\"\"\"\n",
    "    mean_true = image_true.mean()\n",
    "    mean_generated = image_generated.mean()\n",
    "    std_true = image_true.std()\n",
    "    std_generated = image_generated.std()\n",
    "    covariance = (\n",
    "        (image_true - mean_true) * (image_generated - mean_generated)).mean()\n",
    "\n",
    "    numerator = (2 * mean_true * mean_generated + C1) * (2 * covariance + C2)\n",
    "    denominator = ((mean_true ** 2 + mean_generated ** 2 + C1) *\n",
    "                   (std_true ** 2 + std_generated ** 2 + C2))\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXzmSOQWhitm"
   },
   "source": [
    "We will now evaluate the generator with these three metrics on both the\n",
    "training set and the test set by computing the mean value for each metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PQu-2JFWhitm",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_generator(generator):\n",
    "    \"\"\"Evaluate a generator.\n",
    "\n",
    "    Args:\n",
    "        generator: (GeneratorUNet) neural network generating T2-w images\n",
    "\n",
    "    \"\"\"\n",
    "    res_train, res_test = [], []\n",
    "\n",
    "    cuda = True if torch.cuda.is_available() else False\n",
    "    Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for i, batch in enumerate(train_loader):\n",
    "\n",
    "            # Inputs T1-w and T2-w\n",
    "            real_t1 = batch[\"T1\"].type(Tensor)\n",
    "            real_t2 = batch[\"T2\"].type(Tensor)\n",
    "            fake_t2 = generator(real_t1)\n",
    "\n",
    "            mae = mean_absolute_error(real_t2, fake_t2).item()\n",
    "            psnr = peak_signal_to_noise_ratio(real_t2, fake_t2).item()\n",
    "            ssim = structural_similarity_index(real_t2, fake_t2).item()\n",
    "\n",
    "            res_train.append([mae, psnr, ssim])\n",
    "\n",
    "        for i, batch in enumerate(test_loader):\n",
    "\n",
    "            # Inputs T1-w and T2-w\n",
    "            real_t1 = batch[\"T1\"].type(Tensor)\n",
    "            real_t2 = batch[\"T2\"].type(Tensor)\n",
    "            fake_t2 = generator(real_t1)\n",
    "\n",
    "            mae = mean_absolute_error(real_t2, fake_t2).item()\n",
    "            psnr = peak_signal_to_noise_ratio(real_t2, fake_t2).item()\n",
    "            ssim = structural_similarity_index(real_t2, fake_t2).item()\n",
    "\n",
    "            res_test.append([mae, psnr, ssim])\n",
    "\n",
    "    df = pd.DataFrame([\n",
    "        pd.DataFrame(res_train, columns=['MAE', 'PSNR', 'SSIM']).mean().squeeze(),\n",
    "        pd.DataFrame(res_test, columns=['MAE', 'PSNR', 'SSIM']).mean().squeeze()\n",
    "    ], index=['Training set', 'Test set']).T\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J7Z0b1XHhitm",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "evaluate_generator(generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-LIbDqGhitn"
   },
   "source": [
    "The performance is already really good! The task may be pretty easy.\n",
    "Let's see if we can still improve the performance with a more complex neural\n",
    "network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0ipYmtphitn"
   },
   "source": [
    "# 2. Conditional Generative Adversarial Network (cGAN)\n",
    "\n",
    "A generative adversarial network (GAN) is a network generating new samples.\n",
    "A typical GAN consists of two networks:\n",
    "\n",
    "* a **generator** that generates new samples, and\n",
    "* a **discriminator** that discriminate generated samples from true samples.\n",
    "\n",
    "One can think of the generator as a *counterfeiter* and the discriminator\n",
    "as a *authenticator*.\n",
    "The discriminator aims at improving the generator by having an opposition.\n",
    "The discriminator must not be too good, otherwise the generator won't improve.\n",
    "The generator and the discriminator are trained simultaneously and help\n",
    "each other improve.\n",
    "\n",
    "A conditional generative adversarial network (cGAN) is a particular case\n",
    "of a GAN that is conditioned by the true sample.\n",
    "A conditional GAN can thus only be used when **paired samples** are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXZ5peufhitn"
   },
   "source": [
    "## 2.1 Architecture of the cGAN\n",
    "\n",
    "Like a GAN, a cGAN has two networks:\n",
    "\n",
    "* a **generator** that generates new samples, and\n",
    "\n",
    "* a **discriminator** that discriminate generated samples from true samples.\n",
    "\n",
    "We will keep the same architecture for the generator.\n",
    "\n",
    "For the discriminator we will use a **two-dimensional convolutional neural\n",
    "network** with 5 layers:\n",
    "\n",
    "* the first 4 layers are 2D-convolutional layers with  a LeakyReLU activation\n",
    "function;\n",
    "\n",
    "* the last layer is a 2D-convolutional layer.\n",
    "\n",
    "The parameters for each layer are given in the figure below. Don't forget\n",
    "that the input of the discriminator will be the generated image and the true\n",
    "image since we are using a conditional GAN. Therefore, the number of input\n",
    "channels for the first layer will be two (one for each image)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t8P-mSJZhitn"
   },
   "source": [
    "<a href=\"https://ibb.co/9b2jF0V\">\n",
    "  <img src=\"https://i.ibb.co/hBHvPNZ/Capture-d-cran-2020-03-02-16-04-14.png\" width=\"800\"  border=\"0\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tOnJ2Yhhitn"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    " <b>Exercise</b>: Create a <code>Discriminator</code> class to define the\n",
    " discriminator with the architecture given above.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBnOVm7khito",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# We provide a function to generate a block for the given architecture.\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABPma7Zuhito"
   },
   "source": [
    "Let's have a look at the architecture of our discriminator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We provide a function to generate a block for the given architecture.\n",
    "# def discriminator_block(in_filters, out_filters):\n",
    "#     \"\"\"Return downsampling layers of each discriminator block\"\"\"\n",
    "#     layers = [nn.Conv2d(in_filters, out_filters, 3, stride=2, padding=1)]\n",
    "#     layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "# #    return layers\n",
    "\n",
    "# class discriminator_block(nn.Module):\n",
    "#     def __init__(self, in_size, out_size):\n",
    "#         super(discriminator_block, self).__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Conv2d(in_size, out_size, kernel_size=3, stride=2, padding=1),\n",
    "#             nn.LeakyReLU(0.2, inplace=True)\n",
    "#           )\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self, in_channels=1):\n",
    "#         super(Discriminator, self).__init__()\n",
    "#         # To complete\n",
    "#         features_d=64\n",
    "#         self.model = nn.Sequential(\n",
    "#             discriminator_block(in_channels*2,features_d), #img 32*32\n",
    "#             discriminator_block(features_d,features_d*2), #img 16*16\n",
    "#             discriminator_block(features_d*2,features_d*4), #img 8*8\n",
    "#             discriminator_block(features_d*4,features_d*8), #img 4*4\n",
    "#             nn.Conv2d(features_d*8, 1, 1, stride=4, padding=0), #img 4*4\n",
    "# #             nn.Sigmoid()\n",
    "#           )\n",
    "\n",
    "\n",
    "#     def forward(self, img_A, img_B):\n",
    "#         # Concatenate image and condition image by channels to produce input\n",
    "#         # To complete\n",
    "#         x=torch.cat([img_A,img_B], dim=1)\n",
    "#         print(x.shape)\n",
    "#         x = self.model(x)\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of the discriminator\n",
    "# summary(Discriminator().cuda(), [(1,64, 64), (1,64, 64)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_block(in_filters, out_filters):\n",
    "    \"\"\"Return downsampling layers of each discriminator block\"\"\"\n",
    "    layers = [nn.Conv2d(in_filters, out_filters, 3, stride=2, padding=1)]\n",
    "    layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "    return layers\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        layers.extend(discriminator_block(in_channels*2, 64))\n",
    "        layers.extend(discriminator_block(64, 128))\n",
    "        layers.extend(discriminator_block(128, 256))\n",
    "        layers.extend(discriminator_block(256, 512))\n",
    "        layers.append(nn.Conv2d(512, 1, 4, padding=0))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, img):\n",
    "#         return self.model(img)\n",
    "\n",
    "    def forward(self, img_A, img_B):\n",
    "        # Concatenate image and condition image by channels to produce input\n",
    "        # To complete\n",
    "        x=torch.cat([img_A,img_B], dim=1)\n",
    "        print(x.shape)\n",
    "        x = self.model(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of the discriminator\n",
    "summary(Discriminator().cuda(), [(1,64, 64), (1,64, 64)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NPnqeHZhitp"
   },
   "source": [
    "## 2.2 Training our conditional GAN\n",
    "\n",
    "Now that we have created our generator and our discriminator, we have to\n",
    "train them on the dataset.\n",
    "\n",
    "**Notations**\n",
    "\n",
    "* $X_{T1}$: true T1 image;\n",
    "* $X_{T2}$: true T2 image;\n",
    "* $\\tilde{X}_{T2}$: generated T2 image from $X_{T1}$;\n",
    "* $\\hat{y}_{X}$: probability returned by the discriminator that the ${X}_{T2}$ is real;\n",
    "* $\\hat{y}_{\\tilde{X}}$: probability returned by the discriminator that the $\\tilde{X}_{T2}$ is real.\n",
    "\n",
    "**Training the generator**\n",
    "\n",
    "The loss for the generator is the sum of:\n",
    "\n",
    "* the binary cross-entropy loss between the predicted probabilities of the\n",
    "generated images and positive labels,\n",
    "* the pixel-wise mean absolute error between the generated image and the true\n",
    "image.\n",
    "\n",
    "For one sample, it is then:\n",
    "\n",
    "$$\n",
    "\\ell_G = - \\log(\\hat{y}_{\\tilde{X}}) + \\lambda * \\text{MAE}(X_{T2}, \\tilde{X}_{T2})\n",
    "$$\n",
    "\n",
    "**Training the discriminator**\n",
    "\n",
    "The loss for the generator is the mean of:\n",
    "\n",
    "* the binary cross-entropy loss between the predicted probabilities of the\n",
    "generated images and negative labels,\n",
    "* the binary cross-entropy loss between the predicted probabilities\n",
    "of the true images and positive labels.\n",
    "\n",
    "For one sample, it is then:\n",
    "\n",
    "$$\n",
    "\\ell_D = - 0.5 * \\log(\\hat{y}_{X}) - 0.5 * \\log(1 - \\hat{y}_{\\tilde{X}})\n",
    "$$\n",
    "\n",
    "**Training phase**\n",
    "\n",
    "The generator and the discriminator are trained simultaneously, which makes\n",
    "the training phase look like this:\n",
    "\n",
    "```\n",
    "# For each epoch\n",
    "\n",
    "    # For each batch\n",
    "\n",
    "        # Generate fake images for all the images in this batch\n",
    "\n",
    "        # Compute the loss for the generator and perform one optimization step\n",
    "\n",
    "        # Compute the loss for the discriminator and perform one optimization step\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKHda2vWhitp"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Exercise</b>: We provide below a template to train our conditional GAN\n",
    " on the dataset. Fill in the missing parts.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UVkkb1T-hitp",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_cgan(train_loader, test_loader, num_epoch=500,\n",
    "               lr=0.0001, beta1=0.9, beta2=0.999):\n",
    "    \"\"\"Train a conditional GAN.\n",
    "\n",
    "    Args:\n",
    "        train_loader: (DataLoader) a DataLoader wrapping a the training dataset\n",
    "        test_loader: (DataLoader) a DataLoader wrapping a the test dataset\n",
    "        num_epoch: (int) number of epochs performed during training\n",
    "        lr: (float) learning rate of the discriminator and generator Adam optimizers\n",
    "        beta1: (float) beta1 coefficient of the discriminator and generator Adam optimizers\n",
    "        beta2: (float) beta1 coefficient of the discriminator and generator Adam optimizers\n",
    "\n",
    "    Returns:\n",
    "        generator: (nn.Module) the trained generator\n",
    "    \"\"\"\n",
    "\n",
    "    cuda = True if torch.cuda.is_available() else False\n",
    "    print(f\"Using cuda device: {cuda}\")  # check if GPU is used\n",
    "\n",
    "    # Tensor type (put everything on GPU if possible)\n",
    "    Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "    # Output folder\n",
    "    if not os.path.exists(\"./images/cgan\"):\n",
    "        os.makedirs(\"./images/cgan\")\n",
    "\n",
    "    # Loss functions\n",
    "    criterion_GAN = torch.nn.BCEWithLogitsLoss()   # To complete. A loss adapted to binary classification like torch.nn.BCEWithLogitsLoss\n",
    "    criterion_pixelwise = torch.nn.L1Loss()    # To complete. A loss for a voxel-wise comparison of images like torch.nn.L1Loss\n",
    "\n",
    "    lambda_GAN = 1.  # Weights criterion_GAN in the generator loss\n",
    "    lambda_pixel = 1.  # Weights criterion_pixelwise in the generator loss\n",
    "\n",
    "    # Initialize generator and discriminator\n",
    "    generator = GeneratorUNet(1,1)   # To complete\n",
    "    discriminator = Discriminator(1)   # To complete\n",
    "\n",
    "    if cuda:\n",
    "        generator = generator.cuda()\n",
    "        discriminator = discriminator.cuda()\n",
    "        criterion_GAN.cuda()\n",
    "        criterion_pixelwise.cuda()\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_generator = torch.optim.Adam(\n",
    "        generator.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "    optimizer_discriminator = torch.optim.Adam(\n",
    "        discriminator.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "\n",
    "    def sample_images(epoch):\n",
    "        \"\"\"Saves a generated sample from the validation set\"\"\"\n",
    "        imgs = next(iter(test_loader))\n",
    "        real_t1 = imgs[\"T1\"].type(Tensor)\n",
    "        real_t2 = imgs[\"T2\"].type(Tensor)\n",
    "        fake_t2 = generator(real_t1)\n",
    "        img_sample = torch.cat((real_t1.data, fake_t2.data, real_t2.data), -2)\n",
    "        save_image(img_sample, f\"./images/cgan/epoch-{epoch}.png\",\n",
    "                   nrow=5, normalize=True)\n",
    "\n",
    "    # ----------\n",
    "    #  Training\n",
    "    # ----------\n",
    "\n",
    "    prev_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        for i, batch in enumerate(train_loader):\n",
    "\n",
    "            # Inputs T1-w and T2-w\n",
    "            real_t1 = batch[\"T1\"].type(Tensor)\n",
    "            real_t2 = batch[\"T2\"].type(Tensor)\n",
    "\n",
    "            # Create labels\n",
    "            valid = Tensor(np.ones((real_t2.size(0), 1, 1, 1)))\n",
    "            fake = Tensor(np.zeros((real_t2.size(0), 1, 1, 1)))\n",
    "\n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "            optimizer_generator.zero_grad()\n",
    "            #----------------------------------------------------------------\n",
    "\n",
    "            #----------------------------------------------------------------\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            optimizer_discriminator.zero_grad()\n",
    "            \n",
    "            fake_t2 = generator(real_t1)\n",
    "            # Real loss\n",
    "            pred_real = discriminator(real_t2,real_t1)   # To complete\n",
    "            loss_real = criterion_GAN(pred_real,valid)   # To complete\n",
    "\n",
    "            # Fake loss\n",
    "            pred_fake = discriminator(fake_t2,real_t1)   # To complete\n",
    "            loss_fake = criterion_GAN(pred_fake,fake)   # To complete\n",
    "\n",
    "            # Total loss\n",
    "            loss_discriminator = 0.5 * (loss_real + loss_fake)\n",
    "\n",
    "            # Compute the gradient and perform one optimization step\n",
    "            loss_discriminator.backward(retain_graph=True)\n",
    "            optimizer_discriminator.step()\n",
    "\n",
    "            # --------------\n",
    "            #  Log Progress\n",
    "            # --------------\n",
    "            \n",
    "            # GAN loss\n",
    "            fake_t2 = generator(real_t1)   # To complete\n",
    "            pred_fake = discriminator(fake_t2,real_t1)   # To complete\n",
    "            loss_GAN = criterion_GAN(pred_fake,valid)   # To complete\n",
    "\n",
    "            # L1 loss\n",
    "            loss_pixel = criterion_pixelwise(fake_t2,real_t2)   # To complete\n",
    "\n",
    "            # Total loss\n",
    "            loss_generator = lambda_GAN * loss_GAN + lambda_pixel * loss_pixel\n",
    "\n",
    "            # Compute the gradient and perform one optimization step\n",
    "            loss_generator.backward()\n",
    "            optimizer_generator.step()\n",
    "\n",
    "\n",
    "            # Determine approximate time left\n",
    "            batches_done = epoch * len(train_loader) + i\n",
    "            batches_left = num_epoch * len(train_loader) - batches_done\n",
    "            time_left = datetime.timedelta(\n",
    "                seconds=batches_left * (time.time() - prev_time))\n",
    "            prev_time = time.time()\n",
    "\n",
    "            # Print log\n",
    "            sys.stdout.write(\n",
    "                \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] \"\n",
    "                \"[G loss: %f, pixel: %f, adv: %f] ETA: %s\"\n",
    "                % (\n",
    "                    epoch + 1,\n",
    "                    num_epoch,\n",
    "                    i,\n",
    "                    len(train_loader),\n",
    "                    loss_discriminator.item(),\n",
    "                    loss_generator.item(),\n",
    "                    loss_pixel.item(),\n",
    "                    loss_GAN.item(),\n",
    "                    time_left,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Save images at the end of each epoch\n",
    "        sample_images(epoch)\n",
    "\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P7H4TnByhitq",
    "tags": [
     "remove_output"
    ]
   },
   "outputs": [],
   "source": [
    "generator_cgan = train_cgan(train_loader, test_loader, num_epoch=num_epoch,\n",
    "                            lr=lr, beta1=beta1, beta2=beta2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1smygzhNhitr"
   },
   "source": [
    "## 2.3 Evaluating the generator of our cGAN\n",
    "\n",
    "Let's visualize a few generated T2-weighted images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fLiB17ZBhitr"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 20))\n",
    "im = img.imread(f'./images/cgan/epoch-{num_epoch - 1}.png')\n",
    "plt.imshow(np.swapaxes(im, 0, 1))\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0za5qADohits"
   },
   "source": [
    "We will now evaluate the generator of the cGAN with the same three metrics\n",
    "on both the training set and the test set by computing the mean value for\n",
    "each metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DFM6B7-Chits",
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "evaluate_generator(generator_cgan)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "GAN.ipynb",
   "provenance": []
  },
  "jupytext": {
   "cell_metadata_json": true,
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
