{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02ccef4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gayathri/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Discriminator and Generator implementation from DCGAN paper,\n",
    "with removed Sigmoid() as output from Discriminator (and therefor\n",
    "it should be called critic)\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels_img, features_d):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            # input: N x channels_img x 64 x 64\n",
    "            nn.Conv2d(\n",
    "                channels_img, features_d, kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            # _block(in_channels, out_channels, kernel_size, stride, padding)\n",
    "            self._block(features_d, features_d * 2, 4, 2, 1),\n",
    "            self._block(features_d * 2, features_d * 4, 4, 2, 1),\n",
    "            self._block(features_d * 4, features_d * 8, 4, 2, 1),\n",
    "            # After all _block img output is 4x4 (Conv2d below makes into 1x1)\n",
    "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.InstanceNorm2d(out_channels, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, channels_noise, channels_img, features_g):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # Input: N x channels_noise x 1 x 1\n",
    "            self._block(channels_noise, features_g * 16, 4, 1, 0),  # img: 4x4\n",
    "            self._block(features_g * 16, features_g * 8, 4, 2, 1),  # img: 8x8\n",
    "            self._block(features_g * 8, features_g * 4, 4, 2, 1),  # img: 16x16\n",
    "            self._block(features_g * 4, features_g * 2, 4, 2, 1),  # img: 32x32\n",
    "            nn.ConvTranspose2d(\n",
    "                features_g * 2, channels_img, kernel_size=4, stride=2, padding=1\n",
    "            ),\n",
    "            # Output: N x channels_img x 64 x 64\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False,\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "def initialize_weights(model):\n",
    "    # Initializes weights according to the DCGAN paper\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "\n",
    "\n",
    "def test():\n",
    "    N, in_channels, H, W = 8, 3, 64, 64\n",
    "    noise_dim = 100\n",
    "    x = torch.randn((N, in_channels, H, W))\n",
    "    disc = Discriminator(in_channels, 8)\n",
    "    assert disc(x).shape == (N, 1, 1, 1), \"Discriminator test failed\"\n",
    "    gen = Generator(noise_dim, in_channels, 8)\n",
    "    z = torch.randn((N, noise_dim, 1, 1))\n",
    "    assert gen(z).shape == (N, in_channels, H, W), \"Generator test failed\"\n",
    "\n",
    "\n",
    "# test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f76f482",
   "metadata": {},
   "outputs": [],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66b4cb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "   ConvTranspose2d-1             [-1, 16, 4, 4]             256\n",
      "       BatchNorm2d-2             [-1, 16, 4, 4]              32\n",
      "              ReLU-3             [-1, 16, 4, 4]               0\n",
      "   ConvTranspose2d-4              [-1, 8, 8, 8]           2,048\n",
      "       BatchNorm2d-5              [-1, 8, 8, 8]              16\n",
      "              ReLU-6              [-1, 8, 8, 8]               0\n",
      "   ConvTranspose2d-7            [-1, 4, 16, 16]             512\n",
      "       BatchNorm2d-8            [-1, 4, 16, 16]               8\n",
      "              ReLU-9            [-1, 4, 16, 16]               0\n",
      "  ConvTranspose2d-10            [-1, 2, 32, 32]             128\n",
      "      BatchNorm2d-11            [-1, 2, 32, 32]               4\n",
      "             ReLU-12            [-1, 2, 32, 32]               0\n",
      "  ConvTranspose2d-13            [-1, 1, 64, 64]              33\n",
      "             Tanh-14            [-1, 1, 64, 64]               0\n",
      "================================================================\n",
      "Total params: 3,037\n",
      "Trainable params: 3,037\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.15\n",
      "Params size (MB): 0.01\n",
      "Estimated Total Size (MB): 0.16\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# torchsummary and torchvision\n",
    "from torchsummary import summary\n",
    "# from torchvision.utils import save_image\n",
    "summary(Generator(1,1,1).cuda(), [(1,1,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee32fbde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dce302d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/5] Batch 100/938                  Loss D: -1.3613, loss G: 0.6679\n",
      "Epoch [0/5] Batch 200/938                  Loss D: -1.5192, loss G: 0.7401\n",
      "Epoch [0/5] Batch 300/938                  Loss D: -1.5354, loss G: 0.7457\n",
      "Epoch [0/5] Batch 400/938                  Loss D: -1.5360, loss G: 0.7457\n",
      "Epoch [0/5] Batch 500/938                  Loss D: -1.5364, loss G: 0.7457\n",
      "Epoch [0/5] Batch 600/938                  Loss D: -1.5283, loss G: 0.7445\n",
      "Epoch [0/5] Batch 700/938                  Loss D: -1.5338, loss G: 0.7448\n",
      "Epoch [0/5] Batch 800/938                  Loss D: -1.5105, loss G: 0.7414\n",
      "Epoch [0/5] Batch 900/938                  Loss D: -1.4781, loss G: 0.7052\n",
      "Epoch [1/5] Batch 100/938                  Loss D: -1.4643, loss G: 0.7126\n",
      "Epoch [1/5] Batch 200/938                  Loss D: -1.4356, loss G: 0.6758\n",
      "Epoch [1/5] Batch 300/938                  Loss D: -1.3964, loss G: 0.6121\n",
      "Epoch [1/5] Batch 400/938                  Loss D: -1.3614, loss G: 0.6700\n",
      "Epoch [1/5] Batch 500/938                  Loss D: -1.3792, loss G: 0.6690\n",
      "Epoch [1/5] Batch 600/938                  Loss D: -1.3653, loss G: 0.6429\n",
      "Epoch [1/5] Batch 700/938                  Loss D: -1.3649, loss G: 0.6954\n",
      "Epoch [1/5] Batch 800/938                  Loss D: -1.2956, loss G: 0.5606\n",
      "Epoch [1/5] Batch 900/938                  Loss D: -1.3085, loss G: 0.6222\n",
      "Epoch [2/5] Batch 100/938                  Loss D: -1.2022, loss G: 0.4650\n",
      "Epoch [2/5] Batch 200/938                  Loss D: -1.1899, loss G: 0.6699\n",
      "Epoch [2/5] Batch 300/938                  Loss D: -1.1893, loss G: 0.5401\n",
      "Epoch [2/5] Batch 400/938                  Loss D: -1.2222, loss G: 0.5606\n",
      "Epoch [2/5] Batch 500/938                  Loss D: -1.2524, loss G: 0.6401\n",
      "Epoch [2/5] Batch 600/938                  Loss D: -0.9780, loss G: 0.3550\n",
      "Epoch [2/5] Batch 700/938                  Loss D: -1.1863, loss G: 0.5200\n",
      "Epoch [2/5] Batch 800/938                  Loss D: -1.1623, loss G: 0.5996\n",
      "Epoch [2/5] Batch 900/938                  Loss D: -1.2016, loss G: 0.5654\n",
      "Epoch [3/5] Batch 100/938                  Loss D: -0.9300, loss G: 0.6357\n",
      "Epoch [3/5] Batch 200/938                  Loss D: -0.8500, loss G: 0.3045\n",
      "Epoch [3/5] Batch 300/938                  Loss D: -0.9784, loss G: 0.6352\n",
      "Epoch [3/5] Batch 400/938                  Loss D: -1.0837, loss G: 0.6174\n",
      "Epoch [3/5] Batch 500/938                  Loss D: -1.1001, loss G: 0.6123\n",
      "Epoch [3/5] Batch 600/938                  Loss D: -1.1204, loss G: 0.6068\n",
      "Epoch [3/5] Batch 700/938                  Loss D: -1.1331, loss G: 0.5554\n",
      "Epoch [3/5] Batch 800/938                  Loss D: -1.1010, loss G: 0.4610\n",
      "Epoch [3/5] Batch 900/938                  Loss D: -1.0379, loss G: 0.6090\n",
      "Epoch [4/5] Batch 100/938                  Loss D: -1.0217, loss G: 0.3683\n",
      "Epoch [4/5] Batch 200/938                  Loss D: -1.0803, loss G: 0.4914\n",
      "Epoch [4/5] Batch 300/938                  Loss D: -1.0013, loss G: 0.6225\n",
      "Epoch [4/5] Batch 400/938                  Loss D: -1.0401, loss G: 0.5458\n",
      "Epoch [4/5] Batch 500/938                  Loss D: -1.0741, loss G: 0.5713\n",
      "Epoch [4/5] Batch 600/938                  Loss D: -1.0592, loss G: 0.4774\n",
      "Epoch [4/5] Batch 700/938                  Loss D: -1.0125, loss G: 0.3246\n",
      "Epoch [4/5] Batch 800/938                  Loss D: -1.0705, loss G: 0.4672\n",
      "Epoch [4/5] Batch 900/938                  Loss D: -0.9651, loss G: 0.6060\n"
     ]
    }
   ],
   "source": [
    " \"\"\"\n",
    "Training of DCGAN network with WGAN loss\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from model import Discriminator, Generator, initialize_weights\n",
    "\n",
    "# Hyperparameters etc\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "LEARNING_RATE = 5e-5\n",
    "BATCH_SIZE = 64\n",
    "IMAGE_SIZE = 64\n",
    "CHANNELS_IMG = 1\n",
    "Z_DIM = 128\n",
    "NUM_EPOCHS = 5\n",
    "FEATURES_CRITIC = 64\n",
    "FEATURES_GEN = 64\n",
    "CRITIC_ITERATIONS = 5\n",
    "WEIGHT_CLIP = 0.01\n",
    "\n",
    "transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(IMAGE_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = datasets.MNIST(root=\"dataset/\", transform=transforms, download=True)\n",
    "#comment mnist and uncomment below if you want to train on CelebA dataset\n",
    "#dataset = datasets.ImageFolder(root=\"celeb_dataset\", transform=transforms)\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# initialize gen and disc/critic\n",
    "gen = Generator(Z_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\n",
    "critic = Discriminator(CHANNELS_IMG, FEATURES_CRITIC).to(device)\n",
    "initialize_weights(gen)\n",
    "initialize_weights(critic)\n",
    "\n",
    "# initializate optimizer\n",
    "opt_gen = optim.RMSprop(gen.parameters(), lr=LEARNING_RATE)\n",
    "opt_critic = optim.RMSprop(critic.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# for tensorboard plotting\n",
    "fixed_noise = torch.randn(32, Z_DIM, 1, 1).to(device)\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "step = 0\n",
    "\n",
    "gen.train()\n",
    "critic.train()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Target labels not needed! <3 unsupervised\n",
    "    for batch_idx, (data, _) in enumerate(loader):\n",
    "        data = data.to(device)\n",
    "        cur_batch_size = data.shape[0]\n",
    "\n",
    "        # Train Critic: max E[critic(real)] - E[critic(fake)]\n",
    "        for _ in range(CRITIC_ITERATIONS):\n",
    "            noise = torch.randn(cur_batch_size, Z_DIM, 1, 1).to(device)\n",
    "            fake = gen(noise)\n",
    "            critic_real = critic(data).reshape(-1)\n",
    "            critic_fake = critic(fake).reshape(-1)\n",
    "            loss_critic = -(torch.mean(critic_real) - torch.mean(critic_fake))\n",
    "            critic.zero_grad()\n",
    "            loss_critic.backward(retain_graph=True)\n",
    "            opt_critic.step()\n",
    "\n",
    "            # clip critic weights between -0.01, 0.01\n",
    "            for p in critic.parameters():\n",
    "                p.data.clamp_(-WEIGHT_CLIP, WEIGHT_CLIP)\n",
    "\n",
    "        # Train Generator: max E[critic(gen_fake)] <-> min -E[critic(gen_fake)]\n",
    "        gen_fake = critic(fake).reshape(-1)\n",
    "        loss_gen = -torch.mean(gen_fake)\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        # Print losses occasionally and print to tensorboard\n",
    "        if batch_idx % 100 == 0 and batch_idx > 0:\n",
    "            gen.eval()\n",
    "            critic.eval()\n",
    "            print(\n",
    "                f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(loader)} \\\n",
    "                  Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(noise)\n",
    "                # take out (up to) 32 examples\n",
    "                img_grid_real = torchvision.utils.make_grid(\n",
    "                    data[:32], normalize=True\n",
    "                )\n",
    "                img_grid_fake = torchvision.utils.make_grid(\n",
    "                    fake[:32], normalize=True\n",
    "                )\n",
    "\n",
    "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "\n",
    "            step += 1\n",
    "            gen.train()\n",
    "            critic.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7519d9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "TensorBoard 2.10.0a20220522 at http://0.0.0.0:4007/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.autocall.ZMQExitAutocall at 0x7f08592e0250>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=\"\" tensorboard --logdir . --port 4007 --host 0.0.0.0\n",
    "quit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfb3468",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
