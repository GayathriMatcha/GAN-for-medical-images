{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ddacb24",
   "metadata": {},
   "source": [
    "## **Import Libraries:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b927af1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ants\n",
    "import SimpleITK as sitk\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sys.path.insert(0,'./basics/')\n",
    "import basics.slice_view as slice_view\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# torchsummary and torchvision\n",
    "from torchsummary import summary\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# matplotlib stuff\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "\n",
    "# numpy and pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Common python packages\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import h5py\n",
    "import pathlib\n",
    "import random\n",
    "import torch\n",
    "from skimage import feature\n",
    "# from utils import npComplexToTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50531679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device: True\n"
     ]
    }
   ],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "print(f\"Using cuda device: {cuda}\")  # check if GPU is used\n",
    "\n",
    "# Tensor type (put everything on GPU if possible)\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dc8f31",
   "metadata": {},
   "source": [
    "## **Data Loader:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9741901",
   "metadata": {},
   "outputs": [],
   "source": [
    "root='/media/Data/MRI/datasets/'\n",
    "filename1 = '/media/Data/MRI/datasets/mrbrain_t1/cartesian/train/acc_4x/'\n",
    "filename2 = '/media/Data/MRI/datasets/mrbrain_ir/cartesian/train/acc_4x/'\n",
    "filename3 = '/media/Data/MRI/datasets/mrbrain_flair/cartesian/train/acc_4x/'\n",
    "filename11 = '/media/Data/MRI/datasets/mrbrain_t1/cartesian/validation/acc_4x/'\n",
    "filename22 = '/media/Data/MRI/datasets/mrbrain_ir/cartesian/validation/acc_4x/'\n",
    "filename33 = '/media/Data/MRI/datasets/mrbrain_flair/cartesian/validation/acc_4x/'\n",
    "\n",
    "class SliceData(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset that provides access to MR image slices.\n",
    "    \"\"\"\n",
    "\n",
    "    #def __init__(self, root, acc_factor,dataset_type,mask_path): # acc_factor can be passed here and saved as self variable\n",
    "    def __init__(self, root, mode='train'): # acc_factor can be passed here and saved as self variable\n",
    "        # List the h5 files in root \n",
    "        if mode == \"train\":\n",
    "            fn1 = list(pathlib.Path(filename1).iterdir())\n",
    "            fn2 = list(pathlib.Path(filename2).iterdir())\n",
    "            fn3 = list(pathlib.Path(filename3).iterdir())\n",
    "        elif mode == \"test\":\n",
    "            fn1 = list(pathlib.Path(filename11).iterdir())\n",
    "            fn2 = list(pathlib.Path(filename22).iterdir())\n",
    "            fn3 = list(pathlib.Path(filename33).iterdir())\n",
    "        self.examples_t1 = []\n",
    "        self.examples_ir = []\n",
    "        self.examples_flr = []\n",
    "\n",
    "        for fname in sorted(fn1):\n",
    "            with h5py.File(fname,'r') as hf:\n",
    "                fsvol = hf['volfs']\n",
    "                num_slices = fsvol.shape[2]\n",
    "                self.examples_t1 += [(fname, slice) for slice in range(num_slices)]\n",
    "        for fname in sorted(fn2):\n",
    "            with h5py.File(fname,'r') as hf:\n",
    "                fsvol = hf['volfs']\n",
    "                num_slices = fsvol.shape[2]\n",
    "                self.examples_ir += [(fname, slice) for slice in range(num_slices)]\n",
    "        for fname in sorted(fn3):\n",
    "            with h5py.File(fname,'r') as hf:\n",
    "                fsvol = hf['volfs']\n",
    "                num_slices = fsvol.shape[2]\n",
    "                self.examples_flr += [(fname, slice) for slice in range(num_slices)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples_t1)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        fname1, slice = self.examples_t1[i] \n",
    "        fname2, slice = self.examples_ir[i]\n",
    "        fname3, slice = self.examples_flr[i]\n",
    "    \n",
    "        with h5py.File(fname1, 'r') as data:    \n",
    "            t1 = data['volfs'][:,:,slice].astype(np.float64)\n",
    "        with h5py.File(fname2, 'r') as data:    \n",
    "            ir = data['volfs'][:,:,slice].astype(np.float64)\n",
    "        with h5py.File(fname3, 'r') as data:    \n",
    "            flr = data['volfs'][:,:,slice].astype(np.float64)\n",
    "\n",
    "\n",
    "        return torch.from_numpy(t1), torch.from_numpy(ir), torch.from_numpy(flr)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c01fca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader = torch.utils.data.DataLoader(SliceData(root,mode='train'),batch_size=1,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccf257dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i,batch in enumerate(dataloader):\n",
    "#     real_t1 = batch[0]\n",
    "#     c_t1 = torch.zeros((1,3)).cuda()\n",
    "#     c_t1[0][0]=1\n",
    "\n",
    "#     real_ir = batch[1]\n",
    "#     c_ir = torch.zeros((1,3)).cuda()\n",
    "#     c_ir[0][1]=1\n",
    "\n",
    "#     real_flr = batch[2]\n",
    "#     c_flr = torch.zeros((1,3)).cuda()\n",
    "#     c_flr[0][2]=1\n",
    "\n",
    "#     real_t1=real_t1.reshape(240,240)\n",
    "#     real_ir=real_ir.reshape(240,240)\n",
    "#     real_flr=real_flr.reshape(240,240) \n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18bc203b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # real_t1=real_t1.cpu()\n",
    "# real_t1=real_t1.numpy()\n",
    "# # real_flr=real_flr.cpu()\n",
    "# real_ir=real_ir.numpy()\n",
    "# # real_flr=real_flr.cpu()\n",
    "# real_flr=real_flr.numpy()\n",
    "# # real_t1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7122d623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(real_t1,'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beb635e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(real_ir,'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59a29e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(real_flr,'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3421fb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Parameters for Adam optimizer\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "\n",
    "# Create dataloaders\n",
    "# batch_size = 40\n",
    "train_loader =torch.utils.data.DataLoader(SliceData(root,mode='train'),batch_size=1,shuffle=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(SliceData(root,mode='test'),batch_size=1,shuffle=False)\n",
    "\n",
    "# Number of epochs\n",
    "num_epoch = 20\n",
    "\n",
    "# Train the generator\n",
    "# generator = train_generator(train_loader, test_loader, num_epoch=num_epoch,\n",
    "#                              lr=lr, beta1=beta1, beta2=beta2)\n",
    "count=0\n",
    "for i, batch in enumerate(test_loader):\n",
    "\n",
    "    # Inputs T1-w and T2-w\n",
    "    T1 = batch[2] \n",
    "    count+=1\n",
    "#     print(T1.shape)\n",
    "\n",
    "#     break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea4d69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2ccf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# d1=T1.reshape(240,240)\n",
    "# T1_ten=T1.reshape(1,1,240,240)\n",
    "# # d2=d1.reshape(256,256)\n",
    "\n",
    "# # d2=d1.resize(256)\n",
    "# new_image = d1.view((256, 256))\n",
    "# plt.imshow(d1,'gray')\n",
    "# # bts=imge2-im1.transpose()\n",
    "# # T1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8c1ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(d2,'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72821bdb",
   "metadata": {},
   "source": [
    "---\n",
    "# StarGAN:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4ffd5e",
   "metadata": {},
   "source": [
    "## Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "558e2ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from solver import Solver\n",
    "import os\n",
    "import torch\n",
    "# from data_loader import get_loader\n",
    "from torch.backends import cudnn\n",
    "import pandas as pd\n",
    "def str2bool(v):\n",
    "    return v.lower() in ('true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6da8ff0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "48*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdff3bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=1, beta1=0.5, beta2=0.999, c2_dim=3, c_dim=3, celeba_image_dir='/home/gayathri/torchmri/Codes/stargan/images', d_conv_dim=64, d_lr=0.0001, d_repeat_num=6, g_conv_dim=64, g_lr=0.0001, g_repeat_num=6, image_size=240, lambda_cls=1, lambda_gp=10, lambda_rec=10, log_dir='/home/gayathri/torchmri/Codes/stargan/logs/', log_step=100, lr_update_step=10, mode='test', model_save_dir='/home/gayathri/torchmri/Codes/stargan/models', model_save_step=1000, n_critic=5, num_iters=2000, num_iters_decay=1000, num_workers=1, result_dir='/home/gayathri/torchmri/Codes/stargan/results', resume_iters=None, sample_dir='/home/gayathri/torchmri/Codes/stargan/samples', sample_step=500, test_iters=2000, use_tensorboard=True)\n",
      "Generator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(4, 64, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "    (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ResidualBlock(\n",
      "      (main): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (10): ResidualBlock(\n",
      "      (main): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (11): ResidualBlock(\n",
      "      (main): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (12): ResidualBlock(\n",
      "      (main): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (13): ResidualBlock(\n",
      "      (main): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): ResidualBlock(\n",
      "      (main): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (15): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (16): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (19): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(64, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "    (22): Tanh()\n",
      "  )\n",
      ")\n",
      "G\n",
      "The number of parameters: 8411712\n",
      "Discriminator(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (7): LeakyReLU(negative_slope=0.01)\n",
      "    (8): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (9): LeakyReLU(negative_slope=0.01)\n",
      "    (10): Conv2d(1024, 2048, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (11): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (conv1): Conv2d(2048, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  (conv2): Conv2d(2048, 3, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      ")\n",
      "D\n",
      "The number of parameters: 44774336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-24 19:31:32.318512: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-24 19:31:32.319807: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-24 19:31:32.319871: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-24 19:31:32.322063: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-24 19:31:32.322128: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-06-24 19:31:32.322182: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-06-24 19:31:32.322191: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-06-24 19:31:32.322477: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the trained models from step 2000...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/1-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/2-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/3-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/4-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/5-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/6-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/7-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/8-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/9-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/10-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/11-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/12-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/13-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/14-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/15-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/16-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/17-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/18-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/19-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/20-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/21-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/22-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/23-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/24-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/25-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/26-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/27-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/28-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/29-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/30-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/31-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/32-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/33-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/34-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/35-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/36-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/37-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/38-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/39-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/40-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/41-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/42-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/43-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/44-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/45-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/46-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/47-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/48-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/49-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/50-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/51-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/52-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/53-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/54-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/55-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/56-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/57-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/58-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/59-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/60-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/61-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/62-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/63-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/64-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/65-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/66-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/67-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/68-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/69-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/70-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/71-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/72-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/73-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/74-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/75-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/76-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/77-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/78-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/79-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/80-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/81-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/82-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/83-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/84-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/85-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/86-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/87-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/88-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/89-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/90-images.jpg...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/91-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/92-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/93-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/94-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/95-images.jpg...\n",
      "Saved real and fake images into /home/gayathri/torchmri/Codes/stargan/results/96-images.jpg...\n",
      "          Training set   Test set\n",
      "MAE_T1        0.045170   0.047659\n",
      "PSNR_T1      22.721226  22.535418\n",
      "SSIM_T1       0.895179   0.859158\n",
      "MAE_IR        0.692883   0.699052\n",
      "PSNR_IR       3.053520   2.942165\n",
      "SSIM_IR       0.136441   0.140061\n",
      "MAE_FLR       0.044094   0.047631\n",
      "PSNR_FLR     22.393284  22.148080\n",
      "SSIM_FLR      0.883503   0.867764\n"
     ]
    }
   ],
   "source": [
    "def main(config):\n",
    "    # For fast training.\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # Create directories if not exist.\n",
    "    if not os.path.exists(config.log_dir):\n",
    "        os.makedirs(config.log_dir)\n",
    "    if not os.path.exists(config.model_save_dir):\n",
    "        os.makedirs(config.model_save_dir)\n",
    "    if not os.path.exists(config.sample_dir):\n",
    "        os.makedirs(config.sample_dir)\n",
    "    if not os.path.exists(config.result_dir):\n",
    "        os.makedirs(config.result_dir)\n",
    "\n",
    "    # Data loader.\n",
    "    \n",
    "    train_loader =torch.utils.data.DataLoader(SliceData(root,mode='train'),batch_size=1,shuffle=False)\n",
    "    test_loader =torch.utils.data.DataLoader(SliceData(root,mode='test'),batch_size=1,shuffle=False)\n",
    "    \n",
    "\n",
    "    # Solver for training and testing StarGAN.\n",
    "    solver = Solver(train_loader,test_loader,config)\n",
    "\n",
    "    if config.mode == 'train':\n",
    "        solver.train()\n",
    "    elif config.mode == 'test':\n",
    "        solver.test()\n",
    "    df=solver.evaluate()\n",
    "    print(df)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Model configuration.\n",
    "    parser.add_argument('--c_dim', type=int, default=3, help='dimension of domain labels (1st dataset)')\n",
    "    parser.add_argument('--c2_dim', type=int, default=3, help='dimension of domain labels (2nd dataset)')\n",
    "    parser.add_argument('--image_size', type=int, default=240, help='image resolution')\n",
    "    parser.add_argument('--g_conv_dim', type=int, default=64, help='number of conv filters in the first layer of G')\n",
    "    parser.add_argument('--d_conv_dim', type=int, default=64, help='number of conv filters in the first layer of D')\n",
    "    parser.add_argument('--g_repeat_num', type=int, default=6, help='number of residual blocks in G')\n",
    "    parser.add_argument('--d_repeat_num', type=int, default=6, help='number of strided conv layers in D')\n",
    "    parser.add_argument('--lambda_cls', type=float, default=1, help='weight for domain classification loss')\n",
    "    parser.add_argument('--lambda_rec', type=float, default=10, help='weight for reconstruction loss')\n",
    "    parser.add_argument('--lambda_gp', type=float, default=10, help='weight for gradient penalty')\n",
    "    \n",
    "    # Training configuration.\n",
    "    parser.add_argument('--batch_size', type=int, default=1, help='mini-batch size')\n",
    "    parser.add_argument('--num_iters', type=int, default=2000, help='number of total iterations for training D')\n",
    "    parser.add_argument('--num_iters_decay', type=int, default=1000, help='number of iterations for decaying lr')\n",
    "    parser.add_argument('--g_lr', type=float, default=0.0001, help='learning rate for G')\n",
    "    parser.add_argument('--d_lr', type=float, default=0.0001, help='learning rate for D')\n",
    "    parser.add_argument('--n_critic', type=int, default=5, help='number of D updates per each G update')\n",
    "    parser.add_argument('--beta1', type=float, default=0.5, help='beta1 for Adam optimizer')\n",
    "    parser.add_argument('--beta2', type=float, default=0.999, help='beta2 for Adam optimizer')\n",
    "    parser.add_argument('--resume_iters', type=int, default=None, help='resume training from this step')\n",
    "\n",
    "    # Test configuration.\n",
    "    parser.add_argument('--test_iters', type=int, default=2000, help='test model from this step')\n",
    "\n",
    "    # Miscellaneous.\n",
    "    parser.add_argument('--num_workers', type=int, default=1)\n",
    "    parser.add_argument('--mode', type=str, default='test', choices=['train', 'test'])\n",
    "    parser.add_argument('--use_tensorboard', type=str2bool, default=True)\n",
    "\n",
    "    # Directories.\n",
    "    parser.add_argument('--celeba_image_dir', type=str, default='/home/gayathri/torchmri/Codes/stargan/images')\n",
    "#     parser.add_argument('--attr_path', type=str, default='data/celeba/list_attr_celeba.txt')\n",
    "#     parser.add_argument('--rafd_image_dir', type=str, default='data/RaFD/train')\n",
    "    parser.add_argument('--log_dir', type=str, default='/home/gayathri/torchmri/Codes/stargan/logs/')\n",
    "    parser.add_argument('--model_save_dir', type=str, default='/home/gayathri/torchmri/Codes/stargan/models')\n",
    "    parser.add_argument('--sample_dir', type=str, default='/home/gayathri/torchmri/Codes/stargan/samples')\n",
    "    parser.add_argument('--result_dir', type=str, default='/home/gayathri/torchmri/Codes/stargan/results')\n",
    "\n",
    "    # Step size.\n",
    "    parser.add_argument('--log_step', type=int, default=100)\n",
    "    parser.add_argument('--sample_step', type=int, default=500)\n",
    "    parser.add_argument('--model_save_step', type=int, default=1000)\n",
    "    parser.add_argument('--lr_update_step', type=int, default=10)\n",
    "\n",
    "    config = parser.parse_args(args=[])\n",
    "    print(config)\n",
    "    main(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a9bf63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "#         print(x_fixed.shape)\n",
    "# #         print(c_org,c_trg)\n",
    "\n",
    "#         for i in range(start_iters, self.num_iters):\n",
    "\n",
    "#             # =================================================================================== #\n",
    "#             #                             1. Preprocess input data                                #\n",
    "#             # =================================================================================== #\n",
    "#             try:\n",
    "#                 x_fixed_whole = next(data_iter)\n",
    "#             except:\n",
    "#                 data_iter = iter(data_loader)\n",
    "#                 x_fixed_whole = next(data_iter)\n",
    "                \n",
    "            \n",
    "#             real_t1 = x_fixed_whole[0].to(self.device, dtype=torch.float)\n",
    "# #             real_t1 = real_t1.reshape(1,1,240,240)\n",
    "\n",
    "#             # Inputs T1-w and T2-w\n",
    "# #             real_t1 = batch[0].to(self.device, dtype=torch.float)\n",
    "#             c_t1 = torch.zeros((1,3)).to(self.device)\n",
    "#             c_t1[0][0]=1\n",
    "\n",
    "#             real_ir = x_fixed_whole[1].to(self.device, dtype=torch.float)\n",
    "#             c_ir = torch.zeros((1,3)).to(self.device)\n",
    "#             c_ir[0][1]=1\n",
    "\n",
    "#             real_flr = x_fixed_whole[2].to(self.device, dtype=torch.float)\n",
    "#             c_flr = torch.zeros((1,3)).to(self.device)\n",
    "#             c_flr[0][2]=1\n",
    "\n",
    "#             real_t1=real_t1.reshape(1,1,240,240)\n",
    "#             real_ir=real_ir.reshape(1,1,240,240)\n",
    "#             real_flr=real_flr.reshape(1,1,240,240)  \n",
    "\n",
    "#             print(c_ir,c_flr)\n",
    "#             # =================================================================================== #\n",
    "#             #                             2. Train the discriminator                              #\n",
    "#             # =================================================================================== #\n",
    "\n",
    "#             # Compute loss with real images.\n",
    "#             out_src, out_cls = self.D(real_t1)\n",
    "#             d_loss_real = - torch.mean(out_src)\n",
    "#             d_loss_cls = F.binary_cross_entropy_with_logits(out_cls, c_t1, size_average=False) / out_cls.size(0)\n",
    "#             print(out_src,out_cls)\n",
    "#             break\n",
    "# #     break\n",
    "    \n",
    "# #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f097fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_fixed_list =torch.zeros((3,3)).cuda()\n",
    "c_fixed_list[0][0]=1\n",
    "c_fixed_list[1][1]=1\n",
    "c_fixed_list[2][2]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e55dced",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c_fixed in c_fixed_list:\n",
    "    c_fixed=c_fixed.reshape(1,3)\n",
    "    print(c_fixed.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcd4ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd74c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=\"\" tensorboard --logdir . --port 4009 --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4244a19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install --ignore-installed --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44962a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(image_true, image_generated):\n",
    "    return torch.abs(image_true - image_generated).mean()\n",
    "\n",
    "\n",
    "def peak_signal_to_noise_ratio(image_true, image_generated):\n",
    "    mse = ((image_true - image_generated) ** 2).mean().cpu()\n",
    "    return -10 * np.log10(mse)\n",
    "\n",
    "\n",
    "def structural_similarity_index(image_true, image_generated, C1=0.01, C2=0.03):\n",
    "\n",
    "    mean_true = image_true.mean()\n",
    "    mean_generated = image_generated.mean()\n",
    "    std_true = image_true.std()\n",
    "    std_generated = image_generated.std()\n",
    "    covariance = (\n",
    "        (image_true - mean_true) * (image_generated - mean_generated)).mean()\n",
    "\n",
    "    numerator = (2 * mean_true * mean_generated + C1) * (2 * covariance + C2)\n",
    "    denominator = ((mean_true ** 2 + mean_generated ** 2 + C1) *\n",
    "                   (std_true ** 2 + std_generated ** 2 + C2))\n",
    "    return numerator / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d329374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model import Generator\n",
    "# def evaluate_generator(generator):\n",
    "\n",
    "#     res_train, res_test = [], []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "\n",
    "#         for i, batch in enumerate(train_loader):\n",
    "\n",
    "#             # Inputs T1-w and T2-w\n",
    "#             x_t1 = batch[0].to(self.device, dtype=torch.float)\n",
    "#             x_t1 = x_real.reshape(1,1,240,240)\n",
    "#             x_ir=batch[1].to(self.device, dtype=torch.float)\n",
    "#             x_ir=x_real.reshape(1,1,240,240)\n",
    "#             x_flr=batch[2].to(self.device, dtype=torch.float)\n",
    "#             x_flr=x_real.reshape(1,1,240,240)\n",
    "            \n",
    "#             c_trg_list = c_fixed_list=torch.zeros((3,3)).to(self.device)\n",
    "#             c_trg_list[0][0]=1\n",
    "#             c_trg_list[1][1]=1\n",
    "#             c_trg_list[2][2]=1\n",
    "            \n",
    "            \n",
    "# #             fake_t2 = generator(x_t1)\n",
    "#             x_fake_list = []\n",
    "#             for c_trg in c_trg_list:\n",
    "#                 c_trg=c_trg.reshape(1,3)\n",
    "#                 x_fake_list.append(self.G(x_real, c_trg))\n",
    "\n",
    "#             mae_t1 = mean_absolute_error(x_t1, x_fake_list[0]).item()\n",
    "#             psnr_t1 = peak_signal_to_noise_ratio(x_t1, x_fake_list[0]).item()\n",
    "#             ssim_t1 = structural_similarity_index(x_t1, x_fake_list[0]).item()\n",
    "            \n",
    "#             mae_ir = mean_absolute_error(x_ir, x_fake_list[1]).item()\n",
    "#             psnr_ir = peak_signal_to_noise_ratio(x_ir, x_fake_list[1]).item()\n",
    "#             ssim_ir = structural_similarity_index(x_ir, x_fake_list[1]).item()\n",
    "            \n",
    "#             mae_flr = mean_absolute_error(x_flr,x_fake_list[2] ).item()\n",
    "#             psnr_flr = peak_signal_to_noise_ratio(x_flr, x_fake_list[2]).item()\n",
    "#             ssim_flr = structural_similarity_index(x_flr, x_fake_list[2]).item()\n",
    "\n",
    "#             res_train.append([mae_t1, psnr_t1, ssim_t1,mae_ir, psnr_ir, ssim_ir,mae_flr, psnr_flr, ssim_flr])\n",
    "            \n",
    "            \n",
    "\n",
    "#         for i, batch in enumerate(test_loader):\n",
    "\n",
    "#             # Inputs T1-w and T2-w\n",
    "#             x_t1 = batch[0].to(self.device, dtype=torch.float)\n",
    "#             x_t1 = x_real.reshape(1,1,240,240)\n",
    "#             x_ir=batch[1].to(self.device, dtype=torch.float)\n",
    "#             x_ir=x_real.reshape(1,1,240,240)\n",
    "#             x_flr=batch[2].to(self.device, dtype=torch.float)\n",
    "#             x_flr=x_real.reshape(1,1,240,240)\n",
    "            \n",
    "#             c_trg_list = c_fixed_list=torch.zeros((3,3)).to(self.device)\n",
    "#             c_trg_list[0][0]=1\n",
    "#             c_trg_list[1][1]=1\n",
    "#             c_trg_list[2][2]=1\n",
    "            \n",
    "            \n",
    "# #             fake_t2 = generator(x_t1)\n",
    "#             x_fake_list = []\n",
    "#             for c_trg in c_trg_list:\n",
    "#                 c_trg=c_trg.reshape(1,3)\n",
    "#                 x_fake_list.append(self.G(x_real, c_trg))\n",
    "\n",
    "#             mae_t1 = mean_absolute_error(x_t1, x_fake_list[0]).item()\n",
    "#             psnr_t1 = peak_signal_to_noise_ratio(x_t1, x_fake_list[0]).item()\n",
    "#             ssim_t1 = structural_similarity_index(x_t1, x_fake_list[0]).item()\n",
    "            \n",
    "#             mae_ir = mean_absolute_error(x_ir, x_fake_list[1]).item()\n",
    "#             psnr_ir = peak_signal_to_noise_ratio(x_ir, x_fake_list[1]).item()\n",
    "#             ssim_ir = structural_similarity_index(x_ir, x_fake_list[1]).item()\n",
    "            \n",
    "#             mae_flr = mean_absolute_error(x_flr,x_fake_list[2] ).item()\n",
    "#             psnr_flr = peak_signal_to_noise_ratio(x_flr, x_fake_list[2]).item()\n",
    "#             ssim_flr = structural_similarity_index(x_flr, x_fake_list[2]).item()\n",
    "\n",
    "#             res_test.append([mae_t1, psnr_t1, ssim_t1,mae_ir, psnr_ir, ssim_ir,mae_flr, psnr_flr, ssim_flr])\n",
    "\n",
    "#     df = pd.DataFrame([\n",
    "#         pd.DataFrame(res_train, columns=['MAE_T1', 'PSNR_T1', 'SSIM_T1','MAE_IR', 'PSNR_IR', 'SSIM_IR',\n",
    "#                                          'MAE_FLR', 'PSNR_FLR', 'SSIM_FLR']).mean().squeeze(),\n",
    "#         pd.DataFrame(res_test, columns=['MAE_T1', 'PSNR_T1', 'SSIM_T1','MAE_IR', 'PSNR_IR', 'SSIM_IR',\n",
    "#                                          'MAE_FLR', 'PSNR_FLR', 'SSIM_FLR']).mean().squeeze()\n",
    "#     ], index=['Training set', 'Test set']).T\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0618d33e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c090a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_idx = torch.randperm(3)\n",
    "rand_idx\n",
    "label_trg = c[rand_idx]\n",
    "label_trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77a9518",
   "metadata": {},
   "outputs": [],
   "source": [
    "c=torch.tensor([np.array([1, 0, 0]),np.array([0, 1, 0]),np.array([0, 0, 1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a20fae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e100003f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b939468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281b4df1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a86afe99",
   "metadata": {},
   "source": [
    "## Generator Block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513aee0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual Block with instance normalization.\"\"\"\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(dim_in, dim_out, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(dim_out, affine=True, track_running_stats=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(dim_out, dim_out, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(dim_out, affine=True, track_running_stats=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.main(x)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"Generator network.\"\"\"\n",
    "    def __init__(self, conv_dim=64, c_dim=3, repeat_num=6):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Conv2d(1+c_dim, conv_dim, kernel_size=7, stride=1, padding=3, bias=False))\n",
    "        layers.append(nn.InstanceNorm2d(conv_dim, affine=True, track_running_stats=True))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "\n",
    "        # Down-sampling layers.\n",
    "        curr_dim = conv_dim\n",
    "        for i in range(2):\n",
    "            layers.append(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=4, stride=2, padding=1, bias=False))\n",
    "            layers.append(nn.InstanceNorm2d(curr_dim*2, affine=True, track_running_stats=True))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            curr_dim = curr_dim * 2\n",
    "\n",
    "        # Bottleneck layers.\n",
    "        for i in range(repeat_num):\n",
    "            layers.append(ResidualBlock(dim_in=curr_dim, dim_out=curr_dim))\n",
    "\n",
    "        # Up-sampling layers.\n",
    "        for i in range(2):\n",
    "            layers.append(nn.ConvTranspose2d(curr_dim, curr_dim//2, kernel_size=4, stride=2, padding=1, bias=False))\n",
    "            layers.append(nn.InstanceNorm2d(curr_dim//2, affine=True, track_running_stats=True))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "            curr_dim = curr_dim // 2\n",
    "\n",
    "        layers.append(nn.Conv2d(curr_dim, 1, kernel_size=7, stride=1, padding=3, bias=False))\n",
    "        layers.append(nn.Tanh())\n",
    "        self.main = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        # Replicate spatially and concatenate domain information.\n",
    "        # Note that this type of label conditioning does not work at all if we use reflection padding in Conv2d.\n",
    "        # This is because instance normalization ignores the shifting (or bias) effect.\n",
    "        c = c.view(c.size(0), c.size(1), 1, 1)\n",
    "        c = c.repeat(1, 1, x.size(2), x.size(3))\n",
    "        x = torch.cat([x, c], dim=1)\n",
    "        return self.main(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed71e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((1,1,240,240)).cuda()\n",
    "c = torch.zeros((1,3)).cuda()\n",
    "print(c.shape)\n",
    "# c[0,0] = 1.0\n",
    "gen = Generator().cuda()\n",
    "d = gen(x, c)\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a24600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# a = torch.arange(0, 4)\n",
    "c = torch.zeros((1,3))\n",
    "# c=torch.tensor((np.array([1, 0, 0])))\n",
    "c[0][2]=1\n",
    "print(c)\n",
    "print(c.size())\n",
    "# a = a.view(2, 2)\n",
    "# print(a.size())\n",
    "c = c.view(c.size(0), c.size(1), 1, 1)\n",
    "print(c)\n",
    "c = c.repeat(1, 1, 5, 5)\n",
    "print(c)\n",
    "print(c.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241448fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(Generator().cuda(), [(1,240,240),(3,1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba41c85",
   "metadata": {},
   "source": [
    "## **Discriminator:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535bb055",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Discriminator network with PatchGAN.\"\"\"\n",
    "    def __init__(self, image_size=240, conv_dim=64, c_dim=3, repeat_num=6):\n",
    "        super(Discriminator, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Conv2d(1, conv_dim, kernel_size=4, stride=2, padding=1))\n",
    "        layers.append(nn.LeakyReLU(0.01))\n",
    "\n",
    "        curr_dim = conv_dim\n",
    "        for i in range(1, repeat_num):\n",
    "            layers.append(nn.Conv2d(curr_dim, curr_dim*2, kernel_size=4, stride=2, padding=1))\n",
    "            layers.append(nn.LeakyReLU(0.01))\n",
    "            curr_dim = curr_dim * 2\n",
    "\n",
    "        kernel_size = int(image_size / np.power(2, repeat_num))\n",
    "        self.main = nn.Sequential(*layers)\n",
    "        self.conv1 = nn.Conv2d(curr_dim, 1, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(curr_dim, c_dim, kernel_size=kernel_size, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = self.main(x)\n",
    "        out_src = self.conv1(h)\n",
    "        out_cls = self.conv2(h)\n",
    "        return out_src, out_cls.view(out_cls.size(0), out_cls.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1201e5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "int(240/64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09adb18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(Discriminator().cuda(), (1,240,240))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0ff480",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_fixed_list.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83e335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_fixed_list=torch.zeros((3,3))\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012d6507",
   "metadata": {},
   "outputs": [],
   "source": [
    "c[0][0]=1\n",
    "c[1][1]=1\n",
    "c[2][2]=1\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab56402",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eb1e28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93e0ef00",
   "metadata": {},
   "source": [
    "**---------------------------------------------------------------------------------------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138c3087",
   "metadata": {},
   "source": [
    "# CycleGAN:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f604c2",
   "metadata": {},
   "source": [
    "## **Generator Block:**\n",
    "\n",
    "* Architecture:\n",
    "* The generator will have a U-Net architecture with the following characteristics:\n",
    "\n",
    "*   the descending blocks are convolutional layers followed by instance normalization with a LeakyReLU activation function;\n",
    "\n",
    "*    the ascending blocks are transposed convolutional layers followed by instance normalization with a ReLU activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27abb647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We provide classes for each block of the U-Net.\n",
    "\n",
    "class UNetDown(nn.Module):\n",
    "\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(UNetDown, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_size, out_size, kernel_size=3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(out_size),\n",
    "            nn.LeakyReLU(0.2)\n",
    "          )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class UNetUp(nn.Module):\n",
    "\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(UNetUp, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_size, out_size, kernel_size=4,\n",
    "                               stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(out_size),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip_input=None):\n",
    "        if skip_input is not None:\n",
    "            x = torch.cat((x, skip_input), 1)  # add the skip connection\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FinalLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_size, out_size):\n",
    "        super(FinalLayer, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(in_size, out_size, kernel_size=3, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip_input=None):\n",
    "        if skip_input is not None:\n",
    "            x = torch.cat((x, skip_input), 1)  # add the skip connection\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f56047",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorUNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=1):\n",
    "        super(GeneratorUNet, self).__init__()\n",
    "        # To complete\n",
    "        features_g=64\n",
    "\n",
    "        self.und1=UNetDown(in_channels, features_g) #img 64*32*32     #64*120*120\n",
    "        self.und2=UNetDown(features_g, features_g*2) #img 128*16*16   #128*60*60\n",
    "        self.und3=UNetDown(features_g*2, features_g*4) #img 256*8*8   #256*30*30\n",
    "#         self.und4=UNetDown(features_g*4, features_g*8) #img 512*4*4  #512*15*15\n",
    "        \n",
    "        self.mid=nn.Sequential(\n",
    "            nn.Conv2d(features_g*4, features_g*4, kernel_size=3, stride=2, padding=1), #img #512*2x2 #256*15*15\n",
    "            nn.InstanceNorm2d(features_g*4),\n",
    "            nn.ReLU(inplace=True),        \n",
    "        )#512*2*2\n",
    "        \n",
    "        self.unup_1 = UNetUp(features_g*4, features_g*4)#img 512*4*4         #256*30*30\n",
    "#         self.unup_2 = UNetUp(features_g*8*2, features_g*4) #img 256*8*8 \n",
    "        self.unup_3 = UNetUp(features_g*4*2, features_g*2) #img 128*16*16    #128*60*60\n",
    "        self.unup_4 = UNetUp(features_g*2*2, features_g) #img 64*32*32 #     #64*120*120\n",
    "        self.final_layer = FinalLayer(features_g*2,1)#1*64*64\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.und1(x)\n",
    "        x2 = self.und2(x1)\n",
    "        x3 = self.und3(x2)\n",
    "        \n",
    "#         x4 = self.und4(x3)\n",
    "        x = self.mid(x3)\n",
    "        x = self.unup_1(x)\n",
    "#         x = self.unup_2(x,skip_input=x4)\n",
    "        x = self.unup_3(x,skip_input=x3)\n",
    "        x = self.unup_4(x,skip_input=x2)\n",
    "        x = self.final_layer(x,skip_input=x1)      \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e16c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((1,1,240,240)).cuda()\n",
    "c = torch.zeros((1,4)).cuda()\n",
    "# print(c.shape)\n",
    "# c[0,0] = 1.0\n",
    "gen = GeneratorUNet().cuda()\n",
    "d = gen(x)\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e76656b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of the generator\n",
    "summary(GeneratorUNet(1,1).cuda(), (1, 240, 240))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5a1500",
   "metadata": {},
   "source": [
    "## **Discriminator:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c8f64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_block2(in_filters, out_filters):\n",
    "    \"\"\"Return downsampling layers of each discriminator block\"\"\"\n",
    "    layers = [nn.Conv2d(in_filters, out_filters, 3, stride=2, padding=1)]\n",
    "    layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "    return layers\n",
    "class DiscriminatorCycle(nn.Module):\n",
    "    def __init__(self, in_channels=1):\n",
    "        super(DiscriminatorCycle, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        layers.extend(discriminator_block2(in_channels, 64))\n",
    "        layers.extend(discriminator_block2(64, 128))\n",
    "        layers.extend(discriminator_block2(128, 256))\n",
    "        layers.extend(discriminator_block2(256, 512))\n",
    "        layers.append(nn.Conv2d(512, 1, 15, padding=0))\n",
    "#         layers.append(nn.Linear(12,1))\n",
    "#         layers.append(nn.Linear(12,1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bb9e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(DiscriminatorCycle().cuda(), (1, 240, 240))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38000d0d",
   "metadata": {},
   "source": [
    "## Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2881b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cyclegan(train_loader, test_loader, num_epoch=5,\n",
    "                   lr=0.0001, beta1=0.9, beta2=0.999):\n",
    "\n",
    "    cuda = True if torch.cuda.is_available() else False\n",
    "    print(f\"Using cuda device: {cuda}\")  # check if GPU is used\n",
    "\n",
    "    # Tensor type (put everything on GPU if possible)\n",
    "#     Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "    # Output folder\n",
    "    if not os.path.exists(\"./images/cyclegan_mr\"):\n",
    "        os.makedirs(\"./images/cyclegan_mr\")\n",
    "\n",
    "    # Loss functions\n",
    "    criterion_GAN_from_t1_to_t2 = torch.nn.BCEWithLogitsLoss()  # A loss adapted to binary classification like torch.nn.BCEWithLogitsLoss\n",
    "    criterion_GAN_from_t2_to_t1 = torch.nn.BCEWithLogitsLoss()  # A loss adapted to binary classification like torch.nn.BCEWithLogitsLoss\n",
    "    criterion_pixelwise_from_t1_to_t2 = torch.nn.L1Loss()  # A loss for a voxel-wise comparison of images like torch.nn.L1Loss\n",
    "    criterion_pixelwise_from_t2_to_t1 = torch.nn.L1Loss()  # A loss for a voxel-wise comparison of images like torch.nn.L1Loss\n",
    "\n",
    "    lambda_GAN = 1.  # Weights criterion_GAN in the generator loss\n",
    "    lambda_pixel = 1.  # Weights criterion_pixelwise in the generator loss\n",
    "\n",
    "    # Initialize generators and discriminators\n",
    "    generator_from_t1_to_t2 =  GeneratorUNet(1,1)  # To complete\n",
    "    generator_from_t2_to_t1 = GeneratorUNet(1,1)   # To complete \n",
    "    discriminator_from_t1_to_t2 = DiscriminatorCycle(1) # To complete\n",
    "    discriminator_from_t2_to_t1 = DiscriminatorCycle(1)   # To complete\n",
    "\n",
    "    if cuda:\n",
    "        generator_from_t1_to_t2 = generator_from_t1_to_t2.cuda()\n",
    "        generator_from_t2_to_t1 = generator_from_t2_to_t1.cuda()\n",
    "\n",
    "        discriminator_from_t1_to_t2 = discriminator_from_t1_to_t2.cuda()\n",
    "        discriminator_from_t2_to_t1 = discriminator_from_t2_to_t1.cuda()\n",
    "\n",
    "        criterion_GAN_from_t1_to_t2 = criterion_GAN_from_t1_to_t2.cuda()\n",
    "        criterion_GAN_from_t2_to_t1 = criterion_GAN_from_t2_to_t1.cuda()\n",
    "\n",
    "        criterion_pixelwise_from_t1_to_t2 = criterion_pixelwise_from_t1_to_t2.cuda()\n",
    "        criterion_pixelwise_from_t2_to_t1 = criterion_pixelwise_from_t2_to_t1.cuda()\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_generator_from_t1_to_t2 = torch.optim.Adam(\n",
    "        generator_from_t1_to_t2.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "    optimizer_generator_from_t2_to_t1 = torch.optim.Adam(\n",
    "        generator_from_t2_to_t1.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "\n",
    "    optimizer_discriminator_from_t1_to_t2 = torch.optim.Adam(\n",
    "        discriminator_from_t1_to_t2.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "    optimizer_discriminator_from_t2_to_t1 = torch.optim.Adam(\n",
    "        discriminator_from_t2_to_t1.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "\n",
    "    def sample_images(epoch):\n",
    "        \"\"\"Saves a generated sample from the validation set\"\"\"\n",
    "        imgs = next(iter(test_loader))\n",
    "        real_t1 = imgs[0].type(Tensor)\n",
    "        real_t2 = imgs[2].type(Tensor)\n",
    "        real_t1=real_t1.reshape(1,1,240,240)\n",
    "        real_t2=real_t2.reshape(1,1,240,240)\n",
    "        fake_t2 = generator_from_t1_to_t2(real_t1)\n",
    "        img_sample = torch.cat((real_t1.data, fake_t2.data, real_t2.data), -2)\n",
    "        save_image(img_sample, f\"./images/cyclegan_mr/epoch-{epoch}.png\",\n",
    "                   nrow=5, normalize=True)\n",
    "\n",
    "    # ----------\n",
    "    #  Training\n",
    "    # ----------\n",
    "\n",
    "    prev_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        for i, batch in enumerate(train_loader):\n",
    "\n",
    "            # Inputs T1-w and T2-w\n",
    "            real_t1 = batch[0].type(Tensor)\n",
    "            real_t2 = batch[2].type(Tensor)\n",
    "            real_t1=real_t1.reshape(1,1,240,240)\n",
    "            real_t2=real_t2.reshape(1,1,240,240)\n",
    "\n",
    "            # Create labels\n",
    "            valid_t1 = Tensor(np.ones((real_t1.size(0), 1, 1, 1)))\n",
    "            imitation_t1 = Tensor(np.zeros((real_t1.size(0), 1, 1, 1)))\n",
    "\n",
    "            valid_t2 = Tensor(np.ones((real_t2.size(0), 1, 1, 1)))\n",
    "            imitation_t2 = Tensor(np.zeros((real_t2.size(0), 1, 1, 1)))\n",
    "\n",
    " \n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            fake_t1 = generator_from_t2_to_t1(real_t2)   # To complete\n",
    "            fake_t2= generator_from_t1_to_t2(real_t1)   # To complete\n",
    "\n",
    "            optimizer_discriminator_from_t1_to_t2.zero_grad()\n",
    "            optimizer_discriminator_from_t2_to_t1.zero_grad()\n",
    "\n",
    "            # Real loss\n",
    "            pred_real_t2 = discriminator_from_t1_to_t2(real_t2)   # To complete\n",
    "            loss_real_t2 = criterion_GAN_from_t1_to_t2(pred_real_t2,valid_t2)   # To complete\n",
    "\n",
    "            pred_real_t1 = discriminator_from_t2_to_t1(real_t1)  # To complete\n",
    "            loss_real_t1 = criterion_GAN_from_t2_to_t1(pred_real_t1,valid_t1)   # To complete\n",
    "\n",
    "            # Fake loss\n",
    "            pred_fake_t2 = discriminator_from_t1_to_t2(fake_t2)   # To complete\n",
    "            loss_fake_t2 = criterion_GAN_from_t1_to_t2(pred_fake_t2,imitation_t2)   # To complete\n",
    "\n",
    "            pred_fake_t1 = discriminator_from_t2_to_t1(fake_t1)   # To complete\n",
    "            loss_fake_t1 = criterion_GAN_from_t2_to_t1(pred_fake_t1,imitation_t1)   # To complete\n",
    "\n",
    "            # Total loss\n",
    "            loss_discriminator_from_t1_to_t2 = 0.5 * (loss_real_t2 + loss_fake_t2)\n",
    "            loss_discriminator_from_t2_to_t1 = 0.5 * (loss_real_t1 + loss_fake_t1)\n",
    "\n",
    "            loss_discriminator_from_t1_to_t2.backward()\n",
    "            loss_discriminator_from_t2_to_t1.backward()\n",
    "\n",
    "            optimizer_discriminator_from_t1_to_t2.step()\n",
    "            optimizer_discriminator_from_t2_to_t1.step()\n",
    "            \n",
    "                       # ------------------\n",
    "            #  Train Generators\n",
    "            # ------------------\n",
    "            optimizer_generator_from_t1_to_t2.zero_grad()\n",
    "            optimizer_generator_from_t2_to_t1.zero_grad()\n",
    "\n",
    "            # GAN loss\n",
    "            fake_t2 = generator_from_t1_to_t2(real_t1)   # To complete\n",
    "            pred_fake_t2 = discriminator_from_t1_to_t2(fake_t2)   # To complete\n",
    "            loss_GAN_from_t1_to_t2 = criterion_GAN_from_t1_to_t2(pred_fake_t2,valid_t2)   # To complete\n",
    "\n",
    "            fake_t1 = generator_from_t2_to_t1(real_t2)   # To complete\n",
    "            pred_fake_t1 = discriminator_from_t2_to_t1(fake_t1)   # To complete\n",
    "            loss_GAN_from_t2_to_t1 = criterion_GAN_from_t2_to_t1(pred_fake_t1,valid_t1)   # To complete\n",
    "\n",
    "            # L1 loss\n",
    "            fake_fake_t1 = generator_from_t2_to_t1(fake_t2)   # To complete \n",
    "            loss_pixel_from_t1_to_t2 = criterion_pixelwise_from_t1_to_t2(fake_fake_t1,real_t1)   # To complete\n",
    "\n",
    "            fake_fake_t2 = generator_from_t1_to_t2(fake_t1)   # To complete\n",
    "            loss_pixel_from_t2_to_t1 = criterion_pixelwise_from_t2_to_t1(fake_fake_t2,real_t2)   # To complete\n",
    "\n",
    "            # Total loss\n",
    "            loss_generator_from_t1_to_t2 = (lambda_GAN * loss_GAN_from_t1_to_t2 +\n",
    "                                            lambda_pixel * loss_pixel_from_t1_to_t2)\n",
    "            loss_generator_from_t2_to_t1 = (lambda_GAN * loss_GAN_from_t2_to_t1 +\n",
    "                                            lambda_pixel * loss_pixel_from_t2_to_t1)\n",
    "\n",
    "            loss_generator_from_t1_to_t2.backward()\n",
    "            loss_generator_from_t2_to_t1.backward()\n",
    "\n",
    "            optimizer_generator_from_t1_to_t2.step()\n",
    "            optimizer_generator_from_t2_to_t1.step()\n",
    "\n",
    "            # --------------\n",
    "            #  Log Progress\n",
    "            # --------------\n",
    "\n",
    "            # Determine approximate time left\n",
    "            batches_done = epoch * len(train_loader) + i\n",
    "            batches_left = num_epoch * len(train_loader) - batches_done\n",
    "            time_left = datetime.timedelta(\n",
    "                seconds=batches_left * (time.time() - prev_time))\n",
    "            prev_time = time.time()\n",
    "\n",
    "            # Print log\n",
    "            sys.stdout.write(\n",
    "                \"\\r[Epoch %d/%d] [Batch %d/%d] \"\n",
    "                \"[Generator losses: %f, %f] \"\n",
    "                \"[Discriminator losses: %f, %f] \"\n",
    "                \"ETA: %s\"\n",
    "                % (\n",
    "                    epoch + 1,\n",
    "                    num_epoch,\n",
    "                    i,\n",
    "                    len(train_loader),\n",
    "                    loss_generator_from_t1_to_t2.item(),\n",
    "                    loss_generator_from_t2_to_t1.item(),\n",
    "                    loss_discriminator_from_t1_to_t2.item(),\n",
    "                    loss_discriminator_from_t2_to_t1.item(),\n",
    "                    time_left,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Save images at the end of each epoch\n",
    "        sample_images(epoch)\n",
    "\n",
    "    return generator_from_t1_to_t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac25851",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_cyclegan = train_cyclegan(train_loader, test_loader, num_epoch=20,lr=lr, beta1=beta1, beta2=beta2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
