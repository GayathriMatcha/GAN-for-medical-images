{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9657aeaf",
   "metadata": {},
   "source": [
    "# Multi-modal Unsupervised Image-to-Image Translation (MUNIT):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10065bc1",
   "metadata": {},
   "source": [
    "### UNIT\n",
    "\n",
    "UNIT, proposed in [Unsupervised Image-to-Image Translation Networks](https://arxiv.org/abs/1703.00848) (Liu et al. 2018), is a method of image translation that assumes that images from different domains share a latent distribution.\n",
    "\n",
    "Suppose that there are two image domains, $\\mathcal{A}$ and $\\mathcal{B}$. Images $(x_a, x_b) \\in (\\mathcal{A}, \\mathcal{B})$ can be mapped to a shared latent space, $\\mathcal{Z}$ via encoders $E_a: x_a \\mapsto z$ and $E_b: x_b \\mapsto z$, respectively. Synthetic images can be produced via generators $G_a: z \\mapsto x_a'$ and $G_b: z \\mapsto x_b'$, respectively. Note that the generators can generate self-reconstructed or domain-translated images for their respective domains.\n",
    "\n",
    "And as per all other GAN frameworks, synthetic and real images, $(x_a',x_a)$, and $(x_b', x_b)$, are passed into discriminators, $D_a$ and $D_b$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df26d41",
   "metadata": {},
   "source": [
    "### MUNIT\n",
    "\n",
    "Suppose that there are two image domains, $\\mathcal{A}$ and $\\mathcal{B}$. A pair of corresponding images $(x_a, x_b) \\in (\\mathcal{A}, \\mathcal{B})$ can be generated as $x_a = F_a(c, s_a)$ and $x_b = F_b(c, s_b)$ where $c$ is a content vector from a shared distribution, $s_a, s_b$ are style vectors from distinct distributions, and $F_a, F_b$ are decoders that synthesize images from the content and style vectors.\n",
    "\n",
    "The idea is that while the content between two domains can be shared (i.e. you can interchange horses and zebras in an image), the styles are different between the two (i.e. you would draw horses and zebras differently).\n",
    "\n",
    "To learn the content and style distributions in training, the authors also assume some $E_a, E_b$ invert $F_a, F_b$, respectively. Specifically, $E_a^c: x_a \\mapsto c$ extracts content and $E_a^s: x_a \\mapsto s_a$ extracts style from images in domain $\\mathcal{A}$. The same applies for $E_b^c(x_b)$ and $E_b^s(x_b)$ with images in domain $\\mathcal{B}$. You can mix and match the content and style vectors from the two domains to translate images from between the two.\n",
    "\n",
    "For example, if you take content $b$, $c_b = E_b^c(x_b)$, and style $a$, $s_a = E_a^s(x_a)$, and pass these through the horse decoder as $F_a(c_b, s_a)$, you should end up with the image $b$ drawn with characteristics of image $a$.\n",
    "\n",
    "Don't worry if this is still unclear now! You'll go over this in more detail later in the notebook.\n",
    "\n",
    "![Same- and cross-domain interaction of encoders and decoders](https://github.com/https-deeplearning-ai/GANs-Public/blob/master/MUNIT-Domains.png?raw=true)\n",
    "\n",
    "*Model overview, taken from Figure 2 of [Multimodal Unsupervised Image-to-Image Translation](https://arxiv.org/abs/1804.04732) (Huang et al. 2018). The red and blue arrows denote encoders-decoder pairs within the same domain. Left: same domain image reconstruction. Right: cross-domain latent (content and style) vector reconstruction.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785a64cf",
   "metadata": {},
   "source": [
    "## Import Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97098be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "def show_tensor_images(x_real, x_fake):\n",
    "    ''' For visualizing images '''\n",
    "    image_tensor = torch.cat((x_fake[:1, ...], x_real[:1, ...]), dim=0)\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "    image_grid = make_grid(image_unflat, nrow=1)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "import h5py\n",
    "from torchsummary import summary\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# matplotlib stuff\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as img\n",
    "\n",
    "# numpy and pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Common python packages\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import pathlib\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefd1f7d",
   "metadata": {},
   "source": [
    "## **Data_Loader:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3f5e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "root='/media/Data/MRI/datasets/'\n",
    "filename1 = '/media/Data/MRI/datasets/mrbrain_t1/cartesian/train/acc_4x/'\n",
    "filename2 = '/media/Data/MRI/datasets/mrbrain_ir/cartesian/train/acc_4x/'\n",
    "filename3 = '/media/Data/MRI/datasets/mrbrain_flair/cartesian/train/acc_4x/'\n",
    "filename11 = '/media/Data/MRI/datasets/mrbrain_t1/cartesian/validation/acc_4x/'\n",
    "filename22 = '/media/Data/MRI/datasets/mrbrain_ir/cartesian/validation/acc_4x/'\n",
    "filename33 = '/media/Data/MRI/datasets/mrbrain_flair/cartesian/validation/acc_4x/'\n",
    "\n",
    "class SliceData(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset that provides access to MR image slices.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, root, mode='train'): \n",
    "\n",
    "        if mode == \"train\":\n",
    "            fn1 = list(pathlib.Path(filename1).iterdir())\n",
    "            fn2 = list(pathlib.Path(filename2).iterdir())\n",
    "            fn3 = list(pathlib.Path(filename3).iterdir())\n",
    "        elif mode == \"test\":\n",
    "            fn1 = list(pathlib.Path(filename11).iterdir())\n",
    "            fn2 = list(pathlib.Path(filename22).iterdir())\n",
    "            fn3 = list(pathlib.Path(filename33).iterdir())\n",
    "        self.examples_t1 = []\n",
    "        self.examples_ir = []\n",
    "        self.examples_flr = []\n",
    "\n",
    "        for fname in sorted(fn1):\n",
    "            with h5py.File(fname,'r') as hf:\n",
    "                fsvol = hf['volfs']\n",
    "                num_slices = fsvol.shape[2]\n",
    "                self.examples_t1 += [(fname, slice) for slice in range(num_slices)]\n",
    "        for fname in sorted(fn2):\n",
    "            with h5py.File(fname,'r') as hf:\n",
    "                fsvol = hf['volfs']\n",
    "                num_slices = fsvol.shape[2]\n",
    "                self.examples_ir += [(fname, slice) for slice in range(num_slices)]\n",
    "        for fname in sorted(fn3):\n",
    "            with h5py.File(fname,'r') as hf:\n",
    "                fsvol = hf['volfs']\n",
    "                num_slices = fsvol.shape[2]\n",
    "                self.examples_flr += [(fname, slice) for slice in range(num_slices)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples_t1)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        fname1, slice = self.examples_t1[i] \n",
    "        fname2, slice = self.examples_ir[i]\n",
    "        fname3, slice = self.examples_flr[i]\n",
    "    \n",
    "        with h5py.File(fname1, 'r') as data:    \n",
    "            t1 = data['volfs'][:,:,slice].astype(np.float64)\n",
    "        with h5py.File(fname2, 'r') as data:    \n",
    "            ir = data['volfs'][:,:,slice].astype(np.float64)\n",
    "        with h5py.File(fname3, 'r') as data:    \n",
    "            flr = data['volfs'][:,:,slice].astype(np.float64)\n",
    "\n",
    "\n",
    "        return {\"T1\":torch.from_numpy(t1),\"IR\":torch.from_numpy(ir),\"FLR\":torch.from_numpy(flr)}\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ad17d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader =torch.utils.data.DataLoader(SliceData(root,mode='train'),batch_size=1,shuffle=False)\n",
    "for i,batch in enumerate(data_loader):\n",
    "    print(batch['T1'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb9e24d",
   "metadata": {},
   "source": [
    "### Adaptive Instance Normalization (AdaIN)\n",
    "\n",
    "The authors enhance the linear layers for scale and shift with a multi-layer perceptron (MLP), which is essentially just a series of linear layers to help learn more complex representations. See the figure in **Submodules** and the notes in **Submodules: Decoder** for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c052e01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveInstanceNorm2d(nn.Module):\n",
    "    '''\n",
    "    AdaptiveInstanceNorm2d Class\n",
    "    Values:\n",
    "        channels: the number of channels the image has, a scalar\n",
    "        s_dim: the dimension of the style tensor (s), a scalar\n",
    "        h_dim: the hidden dimension of the MLP, a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(self, channels, s_dim=8, h_dim=240):\n",
    "        super().__init__()\n",
    "\n",
    "        self.instance_norm = nn.InstanceNorm2d(channels, affine=False)\n",
    "        self.style_scale_transform = self.mlp(s_dim, h_dim, channels)\n",
    "        self.style_shift_transform = self.mlp(s_dim, h_dim, channels)\n",
    "\n",
    "    @staticmethod\n",
    "    def mlp(self, in_dim, h_dim, out_dim):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(in_dim, h_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(h_dim, h_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(h_dim, out_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, image, w):\n",
    "        '''\n",
    "        Function for completing a forward pass of AdaIN: Given an image and a style, \n",
    "        returns the normalized image that has been scaled and shifted by the style.\n",
    "        Parameters:\n",
    "          image: the feature map of shape (n_samples, channels, width, height)\n",
    "          w: the intermediate noise vector w to be made into the style (y)\n",
    "        '''\n",
    "        normalized_image = self.instance_norm(image)\n",
    "        style_scale = self.style_scale_transform(w)[:, :, None, None]\n",
    "        style_shift = self.style_shift_transform(w)[:, :, None, None]\n",
    "        transformed_image = style_scale * normalized_image + style_shift\n",
    "        return transformed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22642002",
   "metadata": {},
   "source": [
    "### Layer Normalization:\n",
    "* MUNIT uses layer normalization in the upsampling layers of the decoder. Proposed in Layer Normalization (Ba et al. 2016), layer normalization operates similarly to all other normalization techniques like batch normalization, but instead of normalizing across minibatch examples per channel, it normalizes across channels per minibatch example.\n",
    "\n",
    "* Layer normalization is actually much more prevalent in NLP and but quite rare in computer vision. However, batch normalization is not viable here due to training batch sizes of 1 and instance normalization is undesirable because it normalizes the statistics at each position to a standard Gaussian, which removes style features.\n",
    "\n",
    "* Pytorch implements this as nn.LayerNorm but requires precomputed spatial size for initialization to accomodate for 1D, 2D, and 3D inputs. For convenience, let's implement a size-agnostic layer normalization module for 2D inputs (i.e. images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6f89f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm2d(nn.Module):\n",
    "    '''\n",
    "    LayerNorm2d Class\n",
    "    Values:\n",
    "        channels: number of channels in input, a scalar\n",
    "        affine: whether to apply affine denormalization, a bool\n",
    "    '''\n",
    "\n",
    "    def __init__(self, channels, eps=1e-5, affine=True):\n",
    "        super().__init__()\n",
    "        self.affine = affine\n",
    "        self.eps = eps\n",
    "\n",
    "        if self.affine:\n",
    "            self.gamma = nn.Parameter(torch.rand(channels))\n",
    "            self.beta = nn.Parameter(torch.zeros(channels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.flatten(1).mean(1).reshape(-1, 1, 1, 1)\n",
    "        std = x.flatten(1).std(1).reshape(-1, 1, 1, 1)\n",
    "\n",
    "        x = (x - mean) / (std + self.eps)\n",
    "\n",
    "        if self.affine:\n",
    "            x = x * self.gamma.reshape(1, -1, 1, 1) + self.beta.reshape(1, -1, 1, 1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27aaefa",
   "metadata": {},
   "source": [
    "### ResidualBlock:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096565a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    '''\n",
    "    ResidualBlock Class\n",
    "    Values:\n",
    "        channels: number of channels throughout residual block, a scalar\n",
    "        s_dim: the dimension of the style tensor (s), a scalar\n",
    "        h_dim: the hidden dimension of the MLP, a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(self, channels, s_dim=None, h_dim=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.Conv2d(channels, channels, kernel_size=3)\n",
    "            ),\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.Conv2d(channels, channels, kernel_size=3)\n",
    "            ),\n",
    "        )\n",
    "        self.use_style = s_dim is not None and h_dim is not None\n",
    "        if self.use_style:\n",
    "            self.norm1 = AdaptiveInstanceNorm2d(channels, s_dim, h_dim)\n",
    "            self.norm2 = AdaptiveInstanceNorm2d(channels, s_dim, h_dim)\n",
    "        else:\n",
    "            self.norm1 = nn.InstanceNorm2d(channels)\n",
    "            self.norm2 = nn.InstanceNorm2d(channels)\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, s=None):\n",
    "        x_id = x\n",
    "        x = self.conv1(x)\n",
    "        x = self.norm1(x, s) if self.use_style else self.norm1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.norm2(x, s) if self.use_style else self.norm2(x)\n",
    "        return x + x_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fb9c1a",
   "metadata": {},
   "source": [
    "## Submodules: Encoders and Decoder\n",
    "\n",
    "Now that you're all set up and implemented some basic building blocks, let's take a look at the content encoder, style encoder, and decoder! These will be used in the generator.\n",
    "\n",
    "![Same- and cross-domain interaction of encoders and decoders](https://github.com/https-deeplearning-ai/GANs-Public/blob/master/MUNIT-Generator.png?raw=true)\n",
    "\n",
    "*Generator architecture, taken from Figure 3 of [Multimodal Unsupervised Image-to-Image Translation](https://arxiv.org/abs/1804.04732) (Huang et al. 2018). Content encoder: generates a downsampled representation of the original. Style encoder: generates a style code from the original. Decoder: synthesizes a fake image from content code, infused with style info.*\n",
    "\n",
    "*Note: the official implementation feeds style code through a multi-layer perceptron (MLP) and assigns these values to the scale and shift parameters in the instance normalization layers. To be compatible with our previous definitions of* `AdaIN`*, your implementation will simply apply the MLP within* `AdaptiveInstanceNorm2d`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f0b9ed",
   "metadata": {},
   "source": [
    "### Content Encoder\n",
    "\n",
    "The content encoder is similar to many encoders you've already seen: it simply downsamples the input image and feeds it through residual blocks to obtain a condensed representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adca8981",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentEncoder(nn.Module):\n",
    "    '''\n",
    "    ContentEncoder Class\n",
    "    Values:\n",
    "        base_channels: number of channels in first convolutional layer, a scalar\n",
    "        n_downsample: number of downsampling layers, a scalar\n",
    "        n_res_blocks: number of residual blocks, a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(self, base_channels=64, n_downsample=2, n_res_blocks=4):\n",
    "        super().__init__()\n",
    "\n",
    "        channels = base_channels\n",
    "\n",
    "        # Input convolutional layer\n",
    "        layers = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.Conv2d(1, channels, kernel_size=7)\n",
    "            ),\n",
    "            nn.InstanceNorm2d(channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "\n",
    "        # Downsampling layers\n",
    "        for i in range(n_downsample):\n",
    "            layers += [\n",
    "                nn.ReflectionPad2d(1),\n",
    "                nn.utils.spectral_norm(\n",
    "                    nn.Conv2d(channels, 2 * channels, kernel_size=4, stride=2)\n",
    "                ),\n",
    "                nn.InstanceNorm2d(2 * channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            channels *= 2\n",
    "\n",
    "        # Residual blocks\n",
    "        layers += [\n",
    "            ResidualBlock(channels) for _ in range(n_res_blocks)\n",
    "        ]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.out_channels = channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    @property\n",
    "    def channels(self):\n",
    "        return self.out_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d48a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(ContentEncoder().cuda(),(1,240,240))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b30676e",
   "metadata": {},
   "source": [
    "### Style Encoder\n",
    "\n",
    "The style encoder operates similarly to the content encoder but instead of residual blocks, it uses global pooling and fully-connected layers to distill the input image to its style vector. An important difference is that the style encoder doesn't use any normalization layers, since they will remove the feature statistics that encode style. This style code will be passed to the decoder, which use this along with the content code to synthesize a fake image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bb1f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleEncoder(nn.Module):\n",
    "    '''\n",
    "    StyleEncoder Class\n",
    "    Values:\n",
    "        base_channels: number of channels in first convolutional layer, a scalar\n",
    "        n_downsample: number of downsampling layers, a scalar\n",
    "        s_dim: the dimension of the style tensor (s), a scalar\n",
    "    '''\n",
    "\n",
    "    n_deepen_layers = 2\n",
    "\n",
    "    def __init__(self, base_channels=64, n_downsample=4, s_dim=8):\n",
    "        super().__init__()\n",
    "\n",
    "        channels = base_channels\n",
    "\n",
    "        # Input convolutional layer\n",
    "        layers = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.Conv2d(1, channels, kernel_size=7, padding=0)\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ]\n",
    "\n",
    "        # Downsampling layers\n",
    "        for i in range(self.n_deepen_layers):\n",
    "            layers += [\n",
    "                nn.ReflectionPad2d(1),\n",
    "                nn.utils.spectral_norm(\n",
    "                    nn.Conv2d(channels, 2 * channels, kernel_size=4, stride=2)\n",
    "                ),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            channels *= 2\n",
    "        for i in range(n_downsample - self.n_deepen_layers):\n",
    "            layers += [\n",
    "                nn.ReflectionPad2d(1),\n",
    "                nn.utils.spectral_norm(\n",
    "                    nn.Conv2d(channels, channels, kernel_size=4, stride=2)\n",
    "                ),\n",
    "                nn.ReLU(inplace=True),\n",
    "            ]\n",
    "\n",
    "        # Apply global pooling and pointwise convolution to style_channels\n",
    "        layers += [\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(channels, s_dim, kernel_size=1),\n",
    "        ]\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ceb0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(StyleEncoder().cuda(),(1,240,240))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e95f44",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "\n",
    "As with all encoder-decoder frameworks, the decoder serves to synthesize images from the latent information passed through by the encoder. In this case, the decoder works with both content and style encodings.\n",
    "\n",
    "The content encoder and decoder is the backbone of the encoder-decoder framework with style information injected into the residual blocks via `AdaIN` layers.\n",
    "\n",
    "Note: The official implementation feeds style code through a multi-layer perceptron (MLP) and assigns these values to the scale and shift parameters in the instance normalization layers. To be compatible with the previous definitions of `AdaIN` in this course, your implementation will simply apply the MLP within `AdaptiveInstanceNorm2d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596b21d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    '''\n",
    "    Decoder Class\n",
    "    Values:\n",
    "        in_channels: number of channels from encoder output, a scalar\n",
    "        n_upsample: number of upsampling layers, a scalar\n",
    "        n_res_blocks: number of residual blocks, a scalar\n",
    "        s_dim: the dimension of the style tensor (s), a scalar\n",
    "        h_dim: the hidden dimension of the MLP, a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(self, in_channels, n_upsample=2, n_res_blocks=4, s_dim=8, h_dim=240):\n",
    "        super().__init__()\n",
    "\n",
    "        channels = in_channels\n",
    "\n",
    "        # Residual blocks with AdaIN\n",
    "        self.res_blocks = nn.ModuleList([\n",
    "            ResidualBlock(channels, s_dim) for _ in range(n_res_blocks)\n",
    "        ])\n",
    "\n",
    "        # Upsampling blocks\n",
    "        layers = []\n",
    "        for i in range(n_upsample):\n",
    "            layers += [\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                nn.ReflectionPad2d(2),\n",
    "                nn.utils.spectral_norm(\n",
    "                    nn.Conv2d(channels, channels // 2, kernel_size=5)\n",
    "                ),\n",
    "                LayerNorm2d(channels // 2),\n",
    "            ]\n",
    "            channels //= 2\n",
    "        \n",
    "        layers += [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.Conv2d(channels, 1, kernel_size=7)\n",
    "            ),\n",
    "            nn.Tanh(),\n",
    "        ]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, s):\n",
    "        for res_block in self.res_blocks:\n",
    "            x = res_block(x, s=s)\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f642edc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(Decoder(256).cuda(),[(256,60,60),(8,1,1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc7c8de",
   "metadata": {},
   "source": [
    "## **Generator:**\n",
    "\n",
    "The generator is essentially just comprised of the two encoders and one decoder implemented in the previous section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69323957",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    Generator Class\n",
    "    Values:\n",
    "        base_channels: number of channels in first convolutional layer, a scalar\n",
    "        n_downsample: number of downsampling layers, a scalar\n",
    "        n_res_blocks: number of residual blocks, a scalar\n",
    "        s_dim: the dimension of the style tensor (s), a scalar\n",
    "        h_dim: the hidden dimension of the MLP, a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_channels: int = 64,\n",
    "        n_c_downsample: int = 2,\n",
    "        n_s_downsample: int = 4,\n",
    "        n_res_blocks: int = 4,\n",
    "        s_dim: int = 8,\n",
    "#         h_dim: int = 256,\n",
    "        h_dim: int = 240,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.c_enc = ContentEncoder(\n",
    "            base_channels=base_channels, n_downsample=n_c_downsample, n_res_blocks=n_res_blocks,\n",
    "        )\n",
    "        self.s_enc = StyleEncoder(\n",
    "            base_channels=base_channels, n_downsample=n_s_downsample, s_dim=s_dim,\n",
    "        )\n",
    "        self.dec = Decoder(\n",
    "            self.c_enc.channels, n_upsample=n_c_downsample, n_res_blocks=n_res_blocks, s_dim=s_dim, h_dim=h_dim,\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        content = self.c_enc(x)\n",
    "        style = self.s_enc(x)\n",
    "        return (content, style)\n",
    "    \n",
    "    def decode(self, content, style):\n",
    "        return self.dec(content, style)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc4a450",
   "metadata": {},
   "source": [
    "## Discriminator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79041ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    '''\n",
    "    Generator Class\n",
    "    Values:\n",
    "        base_channels: number of channels in first convolutional layer, a scalar\n",
    "        n_layers: number of downsampling layers, a scalar\n",
    "        n_discriminators: number of discriminators (all at different scales), a scalar\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_channels: int = 64,\n",
    "        n_layers: int = 3,\n",
    "        n_discriminators: int = 3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.discriminators = nn.ModuleList([\n",
    "            self.patchgan_discriminator(base_channels, n_layers) for _ in range(n_discriminators)\n",
    "        ])\n",
    "\n",
    "        self.downsample = nn.AvgPool2d(3, stride=2, padding=1, count_include_pad=False)\n",
    "\n",
    "    @staticmethod\n",
    "    def patchgan_discriminator(base_channels, n_layers):\n",
    "        '''\n",
    "        Function that constructs and returns one PatchGAN discriminator module.\n",
    "        '''\n",
    "        channels = base_channels\n",
    "        # Input convolutional layer\n",
    "        layers = [\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.Conv2d(1, channels, kernel_size=4, stride=2),\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        ]\n",
    "\n",
    "        # Hidden convolutional layers\n",
    "        for _ in range(n_layers):\n",
    "            layers += [\n",
    "                nn.ReflectionPad2d(1),\n",
    "                nn.utils.spectral_norm(\n",
    "                    nn.Conv2d(channels, 2 * channels, kernel_size=4, stride=2)\n",
    "                ),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "            ]\n",
    "            channels *= 2\n",
    "\n",
    "        # Output projection layer\n",
    "        layers += [\n",
    "            nn.utils.spectral_norm(\n",
    "                nn.Conv2d(channels, 1, kernel_size=1)\n",
    "            ),\n",
    "        ]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for discriminator in self.discriminators:\n",
    "            outputs.append(discriminator(x))\n",
    "            x = self.downsample(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0cee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(Discriminator().cuda(),(1,240,240))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed09c8b",
   "metadata": {},
   "source": [
    "### Loss Functions\n",
    "\n",
    "There are a lot of moving parts in MUNIT so this section will break down which parts interact with which other parts. Recall from earlier our notation:\n",
    " - Image domains:\n",
    "    \\begin{align*}\n",
    "        a &\\in \\mathcal{A} \\\\\n",
    "        b &\\in \\mathcal{B}\n",
    "    \\end{align*}\n",
    " - Encoders ($E$):\n",
    "    \\begin{align*}\n",
    "        E_a^c: a \\mapsto c_a, &\\quad E_a^s: a \\mapsto s_a \\\\\n",
    "        E_b^c: b \\mapsto c_b, &\\quad E_b^s: b \\mapsto s_b\n",
    "    \\end{align*}\n",
    " - Decoders ($F$):\n",
    "    \\begin{align*}\n",
    "        F_a&: (c_*, s_a) \\mapsto a' \\\\\n",
    "        F_b&: (c_*, s_b) \\mapsto b'\n",
    "    \\end{align*}\n",
    " - Generators ($G$):\n",
    "    \\begin{align*}\n",
    "        G_a(a, b) &= F_a(E_b^c(b), E_a^s(a)) \\\\\n",
    "        G_b(b, a) &= F_b(E_a^c(a), E_b^s(b))\n",
    "    \\end{align*}\n",
    " - Discriminators ($D$):\n",
    "    \\begin{align*}\n",
    "        D_a&: a' \\mapsto p \\in \\mathbb{R} \\\\\n",
    "        D_b&: b' \\mapsto p \\in \\mathbb{R}\n",
    "    \\end{align*}\n",
    "\n",
    "**Image Reconstruction Loss**\n",
    "\n",
    "The model should be able to encode and decode a reconstruction of the image. For domain $\\mathcal{A}$, this can be expressed as\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal{L}_{\\text{recon}}^a &= \\mathbb{E}_{a\\sim p(a)}\\left|\\left|F_a(E_a^c(a), E_a^s(a)) - a\\right|\\right|_1\n",
    "\\end{align*}\n",
    "\n",
    "and for domain $\\mathcal{B}$, this can be expressed as\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal{L}_{\\text{recon}}^b &= \\mathbb{E}_{b\\sim p(b)}\\left|\\left|F_b(E_b^c(b), E_b^s(b)) - b\\right|\\right|_1.\n",
    "\\end{align*}\n",
    "\n",
    "**Latent Reconstruction Loss**\n",
    "\n",
    "The same principle from above applies to the latent space: decoding and encoding a latent vector should reproduce the original. Don't worry if the equations look complicated! Just know that intuitively, passing in different content and style vectors through the encoders should yield those same input vectors. For domain $\\mathcal{A}$, this can be expressed as\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal{L}_{\\text{recon}}^{c_b} &= \\mathbb{E}_{c_b\\sim p(c_b),s_a\\sim q(s_a)}\\left|\\left|E_a^c(F_a(c_b, s_a)) - c_a\\right|\\right|_1 \\\\\n",
    "    \\mathcal{L}_{\\text{recon}}^{s_a} &= \\mathbb{E}_{c_b\\sim p(c_b),s_a\\sim q(s_a)}\\left|\\left|E_a^s(F_a(c_b, s_a)) - s_b\\right|\\right|_1\n",
    "\\end{align*}\n",
    "\n",
    "and for domain $\\mathcal{B}$, this can be expressed as\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal{L}_{\\text{recon}}^{c_a} &= \\mathbb{E}_{c_a\\sim p(c_a),s_b\\sim q(s_b)}\\left|\\left|E_b^c(F_b(c_a, s_b)) - c_b\\right|\\right|_1 \\\\\n",
    "    \\mathcal{L}_{\\text{recon}}^{s_b} &= \\mathbb{E}_{c_a\\sim p(c_a),s_b\\sim q(s_b)}\\left|\\left|E_b^s(F_b(c_a, s_b)) - s_a\\right|\\right|_1\n",
    "\\end{align*}\n",
    "\n",
    "**Adversarial Loss**\n",
    "\n",
    "As with all other GANs, MUNIT is trained with adversarial loss. The authors opt for the LSGAN least-squares objective. For domain $\\mathcal{A}$, this can be expressed as\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal{L}_{\\text{GAN}}^a &= \\mathbb{E}_{c_b\\sim p(c_b),s_a\\sim q(s_a)}\\left[(1 - D_a(G_a(c_b, s_a)))^2\\right] + \\mathbb{E}_{a\\sim p(a)}\\left[D_a(a)^2\\right]\n",
    "\\end{align*}\n",
    "\n",
    "and for domain $\\mathcal{B}$, this can be expressed as\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal{L}_{\\text{GAN}}^b &= \\mathbb{E}_{c_a\\sim p(c_a),s_b\\sim q(s_b)}\\left[(1 - D_b(G_b(c_a, s_b)))^2\\right] + \\mathbb{E}_{b\\sim p(b)}\\left[D_b(b)^2\\right]\n",
    "\\end{align*}\n",
    "\n",
    "**Total Loss**\n",
    "\n",
    "The total loss can now be expressed in terms of the individual losses from above, so the objective can be expressed as\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathcal{L}(E_a, E_b, F_a, F_b, D_a, D_b) &= \\mathcal{L}_{\\text{GAN}}^a + \\mathcal{L}_{\\text{GAN}}^b + \\lambda_x(\\mathcal{L}_{\\text{recon}}^a + \\mathcal{L}_{\\text{recon}}^b) + \\lambda_c(\\mathcal{L}_{\\text{recon}}^{c_a} + \\mathcal{L}_{\\text{recon}}^{c_b}) + \\lambda_s(\\mathcal{L}_{\\text{recon}}^{s_a} + \\mathcal{L}_{\\text{recon}}^{s_b})\n",
    "\\end{align*}\n",
    "\n",
    "And now the fun part: implementing this ginormous composite loss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0be212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GinormousCompositeLoss(nn.Module):\n",
    "    '''\n",
    "    GinormousCompositeLoss Class: implements all losses for MUNIT\n",
    "    '''\n",
    "\n",
    "    @staticmethod\n",
    "    def image_recon_loss(x, gen):\n",
    "        c, s = gen.encode(x)\n",
    "        print(c.shape,s.shape)\n",
    "        recon = gen.decode(c, s)\n",
    "        print(recon.shape)\n",
    "        return F.l1_loss(recon, x), c, s\n",
    "\n",
    "    @staticmethod\n",
    "    def latent_recon_loss(c, s, gen):\n",
    "        x_fake = gen.decode(c, s)\n",
    "        recon = gen.encode(x_fake)\n",
    "        return F.l1_loss(recon[0], c), F.l1_loss(recon[1], s), x_fake\n",
    "\n",
    "    @staticmethod\n",
    "    def adversarial_loss(x, dis, is_real):\n",
    "        preds = dis(x)\n",
    "        target = torch.ones_like if is_real else torch.zeros_like\n",
    "        loss = 0.0\n",
    "        for pred in preds:\n",
    "            loss += F.mse_loss(pred, target(pred))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff3c231",
   "metadata": {},
   "source": [
    "**Style-augmented Cycle Consistency**\n",
    "\n",
    "You've already heard of cycle consistency from CycleGAN, which implies that an image translated to the target domain and back should be identical to the original.\n",
    "\n",
    "Intuitively, style-augmented cycle consistency implies that an image translated to the target domain and back using the original style should result in the original image. Style-augmented cycle consistency is implicitly encouraged\n",
    "by the reconstruction losses, but the authors note that explicitly enforcing it could be useful in some cases.\n",
    "\n",
    "**Domain Invariant Perceptual Loss**\n",
    "\n",
    "You're probably already familiar with perceptual loss, which is usually implemented via MSE loss between feature maps of fake and real images. However, because the images in the domains are unpaired, pixel-wise loss may not be optimal, since each values at each position do not correspond spatially.\n",
    "\n",
    "The authors get around this discrepancy by applying instance normalization to the feature maps. This normalizes the values per channel for each position in the feature maps, so MSE loss penalizes the difference in statistics rather than raw pixel value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94d1b7a",
   "metadata": {},
   "source": [
    "## **Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4838e33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MUNIT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        gen_channels: int = 64,\n",
    "        n_c_downsample: int = 2,\n",
    "        n_s_downsample: int = 4,\n",
    "        n_res_blocks: int = 4,\n",
    "        s_dim: int = 8,\n",
    "#         h_dim: int = 256,\n",
    "        h_dim: int = 240,\n",
    "        dis_channels: int = 64,\n",
    "        n_layers: int = 3,\n",
    "        n_discriminators: int = 3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.gen_a = Generator(\n",
    "            base_channels=gen_channels, n_c_downsample=n_c_downsample, n_s_downsample=n_s_downsample, n_res_blocks=n_res_blocks, s_dim=s_dim, h_dim=h_dim,\n",
    "        )\n",
    "        self.gen_b = Generator(\n",
    "            base_channels=gen_channels, n_c_downsample=n_c_downsample, n_s_downsample=n_s_downsample, n_res_blocks=n_res_blocks, s_dim=s_dim, h_dim=h_dim,\n",
    "        )\n",
    "        self.dis_a = Discriminator(\n",
    "            base_channels=dis_channels, n_layers=n_layers, n_discriminators=n_discriminators,\n",
    "        )\n",
    "        self.dis_b = Discriminator(\n",
    "            base_channels=dis_channels, n_layers=n_layers, n_discriminators=n_discriminators,\n",
    "        )\n",
    "        self.s_dim = s_dim\n",
    "        self.loss = GinormousCompositeLoss\n",
    "\n",
    "    def forward(self, x_a, x_b):\n",
    "        s_a = torch.randn(x_a.size(0), self.s_dim, 1, 1, device=x_a.device).to(x_a.dtype)\n",
    "        s_b = torch.randn(x_b.size(0), self.s_dim, 1, 1, device=x_b.device).to(x_b.dtype)\n",
    "\n",
    "        # Encode real x and compute image reconstruction loss\n",
    "        x_a_loss, c_a, s_a_fake = self.loss.image_recon_loss(x_a, self.gen_a)\n",
    "        x_b_loss, c_b, s_b_fake = self.loss.image_recon_loss(x_b, self.gen_b)\n",
    "\n",
    "        # Decode real (c, s) and compute latent reconstruction loss\n",
    "        c_b_loss, s_a_loss, x_ba = self.loss.latent_recon_loss(c_b, s_a, self.gen_a)\n",
    "        c_a_loss, s_b_loss, x_ab = self.loss.latent_recon_loss(c_a, s_b, self.gen_b)\n",
    "\n",
    "        # Compute adversarial losses\n",
    "        gen_a_adv_loss = self.loss.adversarial_loss(x_ba, self.dis_a, True)\n",
    "        gen_b_adv_loss = self.loss.adversarial_loss(x_ab, self.dis_b, True)\n",
    "\n",
    "        # Sum up losses for gen\n",
    "        gen_loss = (\n",
    "            10 * x_a_loss + c_b_loss + s_a_loss + gen_a_adv_loss + \\\n",
    "            10 * x_b_loss + c_a_loss + s_b_loss + gen_b_adv_loss\n",
    "        )\n",
    "\n",
    "        # Sum up losses for dis\n",
    "        dis_loss = (\n",
    "            self.loss.adversarial_loss(x_ba.detach(), self.dis_a, False) + \\\n",
    "            self.loss.adversarial_loss(x_a.detach(), self.dis_a, True) + \\\n",
    "            self.loss.adversarial_loss(x_ab.detach(), self.dis_b, False) + \\\n",
    "            self.loss.adversarial_loss(x_b.detach(), self.dis_b, True)\n",
    "        )\n",
    "\n",
    "        return gen_loss, dis_loss, x_ab, x_ba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee19eaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(MUNIT().cuda(),[(1,240,240),(1,240,240)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55041c50",
   "metadata": {},
   "source": [
    "## Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6656cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')\n",
    "\n",
    "munit_config = {\n",
    "    'gen_channels': 64,\n",
    "    'n_c_downsample': 2,\n",
    "    'n_s_downsample': 4,\n",
    "    'n_res_blocks': 4,\n",
    "    's_dim': 8,\n",
    "#     'h_dim': 256,\n",
    "    'h_dim': 240,\n",
    "    'dis_channels': 64,\n",
    "    'n_layers': 3,\n",
    "    'n_discriminators': 3,\n",
    "}\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "munit = MUNIT(**munit_config).to(device).apply(weights_init)\n",
    "\n",
    "# Initialize dataloader\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize(286),\n",
    "#     transforms.RandomCrop(256),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5), (0.5))\n",
    "# ])\n",
    "# dataloader = DataLoader(\n",
    "#     ImageDataset('horse2zebra', transform),\n",
    "#     batch_size=1, pin_memory=True, shuffle=True,\n",
    "# )\n",
    "\n",
    "train_loader =torch.utils.data.DataLoader(SliceData(root,mode='train'),batch_size=1,shuffle=False)\n",
    "\n",
    "# Initialize optimizers\n",
    "gen_params = list(munit.gen_a.parameters()) + list(munit.gen_b.parameters())\n",
    "dis_params = list(munit.dis_a.parameters()) + list(munit.dis_b.parameters())\n",
    "gen_optimizer = torch.optim.Adam(gen_params, lr=1e-4, betas=(0.5, 0.999))\n",
    "dis_optimizer = torch.optim.Adam(dis_params, lr=1e-4, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2cb843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse torch version for autocast\n",
    "# ######################################################\n",
    "version = torch.__version__\n",
    "version = tuple(int(n) for n in version.split('.')[:-1])\n",
    "has_autocast = version >= (1, 6)\n",
    "# ######################################################\n",
    "\n",
    "def train(munit, dataloader, optimizers, device):\n",
    "\n",
    "    max_iters = 1000 #000\n",
    "    decay_every = 100 #000\n",
    "    cur_iter = 0\n",
    "\n",
    "    display_every = 500\n",
    "    mean_losses = [0., 0.]\n",
    "\n",
    "    while cur_iter < max_iters:\n",
    "        for i,batch in enumerate(dataloader):\n",
    "            \n",
    "#             x_a = x_a.to(device)\n",
    "#             x_b = x_b.to(device)\n",
    "            \n",
    "            x_a = batch['T1'].to(device, dtype=torch.float16)\n",
    "            print(x_a.shape)\n",
    "#         self.to(torch.float16\n",
    "            x_b = batch['FLR'].to(device, dtype=torch.float16)\n",
    "\n",
    "            # Enable autocast to FP16 tensors (new feature since torch==1.6.0)\n",
    "            # If you're running older versions of torch, comment this out\n",
    "            # and use NVIDIA apex for mixed/half precision training\n",
    "            if has_autocast:\n",
    "                with torch.cuda.amp.autocast(enabled=(device=='cuda')):\n",
    "                    outputs = munit(x_a, x_b)\n",
    "            else:\n",
    "                outputs = munit(x_a, x_b)\n",
    "            \n",
    "            losses, x_ab, x_ba = outputs[:-2], outputs[-2], outputs[-1]\n",
    "            munit.zero_grad()\n",
    "\n",
    "            for i, (optimizer, loss) in enumerate(zip(optimizers, losses)):\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                mean_losses[i] += loss.item() / display_every\n",
    "\n",
    "            cur_iter += 1\n",
    "\n",
    "            if cur_iter % display_every == 0:\n",
    "                print('Step {}: [G loss: {:.5f}][D loss: {:.5f}]'\n",
    "                      .format(cur_iter, *mean_losses))\n",
    "                show_tensor_images(x_ab, x_a)\n",
    "                show_tensor_images(x_ba, x_b)\n",
    "                mean_losses = [0., 0.]\n",
    "\n",
    "            if cur_iter == max_iters:\n",
    "                break\n",
    "\n",
    "            # Schedule learning rate by 0.5\n",
    "            if cur_iter % decay_every == 0:\n",
    "                for optimizer in optimizers:\n",
    "                    for param_group in optimizer.param_groups:\n",
    "                        param_group['lr'] *= 0.5\n",
    "\n",
    "train(\n",
    "    munit, train_loader,\n",
    "    [gen_optimizer, dis_optimizer],\n",
    "    device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cfbfa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
